<!DOCTYPE html>
<html lang="en-US" dir="ltr">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width,initial-scale=1">
    <title>Open Source AI | 2024 COSR</title>
    <meta name="description" content="2024 China Open Source Report">
    <meta name="generator" content="VitePress v1.0.0-rc.36">
    <link rel="preload stylesheet" href="/2024-China-Open-Source-Report/assets/style.DacjpNVy.css" as="style">
    
    <script type="module" src="/2024-China-Open-Source-Report/assets/app.Do7UtULK.js"></script>
    <link rel="preload" href="/2024-China-Open-Source-Report/assets/inter-roman-latin.Bu8hRsVA.woff2" as="font" type="font/woff2" crossorigin="">
    <link rel="modulepreload" href="/2024-China-Open-Source-Report/assets/chunks/theme.Y4pXzSuA.js">
    <link rel="modulepreload" href="/2024-China-Open-Source-Report/assets/chunks/framework.CIQteYsG.js">
    <link rel="modulepreload" href="/2024-China-Open-Source-Report/assets/chunks/HJgpVRwQOyx.DnXV_VFv.js">
    <link rel="modulepreload" href="/2024-China-Open-Source-Report/assets/en_ossAI.md.CnTjTpiv.lean.js">
    <link rel="icon" type="image/x-icon" href="/image/China-Open-Source-Report.ico">
    <link rel="icon" type="image/x-icon" href="/2024-China-Open-Source-Report/image/China-Open-Source-Report.ico">
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-7CSQ2KPB1F"></script>
    <script>window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-7CSQ2KPB1F");</script>
    <script id="check-dark-mode">(()=>{const e=localStorage.getItem("vitepress-theme-appearance")||"auto",a=window.matchMedia("(prefers-color-scheme: dark)").matches;(!e||e==="auto"?a:e==="dark")&&document.documentElement.classList.add("dark")})();</script>
    <script id="check-mac-os">document.documentElement.classList.toggle("mac",/Mac|iPhone|iPod|iPad/i.test(navigator.platform));</script>
  </head>
  <body>
    <div id="app"><div class="Layout" data-v-4709876f><!--[--><!--]--><!--[--><span tabindex="-1" data-v-f828554e></span><a href="#VPContent" class="VPSkipLink visually-hidden" data-v-f828554e> Skip to content </a><!--]--><!----><header class="VPNav" data-v-4709876f data-v-afee977f><div class="VPNavBar" data-v-afee977f data-v-2298dee3><div class="wrapper" data-v-2298dee3><div class="container" data-v-2298dee3><div class="title" data-v-2298dee3><div class="VPNavBarTitle has-sidebar" data-v-2298dee3 data-v-fa91a43f><a class="title" href="/2024-China-Open-Source-Report/en/" data-v-fa91a43f><!--[--><!--]--><!--[--><img class="VPImage logo" src="/2024-China-Open-Source-Report/image/China-Open-Source-Report.png" alt data-v-5420fb09><!--]--><!--[-->2024 COSR<!--]--><!--[--><!--]--></a></div></div><div class="content" data-v-2298dee3><div class="content-body" data-v-2298dee3><!--[--><!--]--><div class="VPNavBarSearch search" data-v-2298dee3><!--[--><!----><div id="local-search"><button type="button" class="DocSearch DocSearch-Button" aria-label="Search"><span class="DocSearch-Button-Container"><svg class="DocSearch-Search-Icon" width="20" height="20" viewBox="0 0 20 20" aria-label="search icon"><path d="M14.386 14.386l4.0877 4.0877-4.0877-4.0877c-2.9418 2.9419-7.7115 2.9419-10.6533 0-2.9419-2.9418-2.9419-7.7115 0-10.6533 2.9418-2.9419 7.7115-2.9419 10.6533 0 2.9419 2.9418 2.9419 7.7115 0 10.6533z" stroke="currentColor" fill="none" fill-rule="evenodd" stroke-linecap="round" stroke-linejoin="round"></path></svg><span class="DocSearch-Button-Placeholder">Search</span></span><span class="DocSearch-Button-Keys"><kbd class="DocSearch-Button-Key"></kbd><kbd class="DocSearch-Button-Key">K</kbd></span></button></div><!--]--></div><nav aria-labelledby="main-nav-aria-label" class="VPNavBarMenu menu" data-v-2298dee3 data-v-480f2cb3><span id="main-nav-aria-label" class="visually-hidden" data-v-480f2cb3>Main Navigation</span><!--[--><!--[--><a class="VPLink link VPNavBarMenuLink" href="/2024-China-Open-Source-Report/en.html" tabindex="0" data-v-480f2cb3 data-v-edb5e7e3><!--[--><span data-v-edb5e7e3>Home</span><!--]--></a><!--]--><!--[--><a class="VPLink link vp-external-link-icon VPNavBarMenuLink" href="https://kaiyuanshe.feishu.cn/wiki/wikcnUDeVll6PNzw900yPV71Sxd" target="_blank" rel="noreferrer" tabindex="0" data-v-480f2cb3 data-v-edb5e7e3><!--[--><span data-v-edb5e7e3>Annual report of previous years</span><!--]--></a><!--]--><!--]--></nav><div class="VPFlyout VPNavBarTranslations translations" data-v-2298dee3 data-v-f9c9ed07 data-v-7946f491><button type="button" class="button" aria-haspopup="true" aria-expanded="false" aria-label="Change language" data-v-7946f491><span class="text" data-v-7946f491><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" viewbox="0 0 24 24" class="option-icon" data-v-7946f491><path d="M0 0h24v24H0z" fill="none"></path><path d=" M12.87 15.07l-2.54-2.51.03-.03c1.74-1.94 2.98-4.17 3.71-6.53H17V4h-7V2H8v2H1v1.99h11.17C11.5 7.92 10.44 9.75 9 11.35 8.07 10.32 7.3 9.19 6.69 8h-2c.73 1.63 1.73 3.17 2.98 4.56l-5.09 5.02L4 19l5-5 3.11 3.11.76-2.04zM18.5 10h-2L12 22h2l1.12-3h4.75L21 22h2l-4.5-12zm-2.62 7l1.62-4.33L19.12 17h-3.24z " class="css-c4d79v"></path></svg><!----><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" viewbox="0 0 24 24" class="text-icon" data-v-7946f491><path d="M12,16c-0.3,0-0.5-0.1-0.7-0.3l-6-6c-0.4-0.4-0.4-1,0-1.4s1-0.4,1.4,0l5.3,5.3l5.3-5.3c0.4-0.4,1-0.4,1.4,0s0.4,1,0,1.4l-6,6C12.5,15.9,12.3,16,12,16z"></path></svg></span></button><div class="menu" data-v-7946f491><div class="VPMenu" data-v-7946f491 data-v-93f7c741><!----><!--[--><!--[--><div class="items" data-v-f9c9ed07><p class="title" data-v-f9c9ed07>English</p><!--[--><div class="VPMenuLink" data-v-f9c9ed07 data-v-8775611d><a class="VPLink link" href="/2024-China-Open-Source-Report/ossAI.html" data-v-8775611d><!--[-->简体中文<!--]--></a></div><!--]--></div><!--]--><!--]--></div></div></div><div class="VPNavBarAppearance appearance" data-v-2298dee3 data-v-5d8568ee><button class="VPSwitch VPSwitchAppearance" type="button" role="switch" title="Switch to dark theme" aria-checked="false" data-v-5d8568ee data-v-8457bf22 data-v-ade1a38d><span class="check" data-v-ade1a38d><span class="icon" data-v-ade1a38d><!--[--><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" viewbox="0 0 24 24" class="sun" data-v-8457bf22><path d="M12,18c-3.3,0-6-2.7-6-6s2.7-6,6-6s6,2.7,6,6S15.3,18,12,18zM12,8c-2.2,0-4,1.8-4,4c0,2.2,1.8,4,4,4c2.2,0,4-1.8,4-4C16,9.8,14.2,8,12,8z"></path><path d="M12,4c-0.6,0-1-0.4-1-1V1c0-0.6,0.4-1,1-1s1,0.4,1,1v2C13,3.6,12.6,4,12,4z"></path><path d="M12,24c-0.6,0-1-0.4-1-1v-2c0-0.6,0.4-1,1-1s1,0.4,1,1v2C13,23.6,12.6,24,12,24z"></path><path d="M5.6,6.6c-0.3,0-0.5-0.1-0.7-0.3L3.5,4.9c-0.4-0.4-0.4-1,0-1.4s1-0.4,1.4,0l1.4,1.4c0.4,0.4,0.4,1,0,1.4C6.2,6.5,5.9,6.6,5.6,6.6z"></path><path d="M19.8,20.8c-0.3,0-0.5-0.1-0.7-0.3l-1.4-1.4c-0.4-0.4-0.4-1,0-1.4s1-0.4,1.4,0l1.4,1.4c0.4,0.4,0.4,1,0,1.4C20.3,20.7,20,20.8,19.8,20.8z"></path><path d="M3,13H1c-0.6,0-1-0.4-1-1s0.4-1,1-1h2c0.6,0,1,0.4,1,1S3.6,13,3,13z"></path><path d="M23,13h-2c-0.6,0-1-0.4-1-1s0.4-1,1-1h2c0.6,0,1,0.4,1,1S23.6,13,23,13z"></path><path d="M4.2,20.8c-0.3,0-0.5-0.1-0.7-0.3c-0.4-0.4-0.4-1,0-1.4l1.4-1.4c0.4-0.4,1-0.4,1.4,0s0.4,1,0,1.4l-1.4,1.4C4.7,20.7,4.5,20.8,4.2,20.8z"></path><path d="M18.4,6.6c-0.3,0-0.5-0.1-0.7-0.3c-0.4-0.4-0.4-1,0-1.4l1.4-1.4c0.4-0.4,1-0.4,1.4,0s0.4,1,0,1.4l-1.4,1.4C18.9,6.5,18.6,6.6,18.4,6.6z"></path></svg><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" viewbox="0 0 24 24" class="moon" data-v-8457bf22><path d="M12.1,22c-0.3,0-0.6,0-0.9,0c-5.5-0.5-9.5-5.4-9-10.9c0.4-4.8,4.2-8.6,9-9c0.4,0,0.8,0.2,1,0.5c0.2,0.3,0.2,0.8-0.1,1.1c-2,2.7-1.4,6.4,1.3,8.4c2.1,1.6,5,1.6,7.1,0c0.3-0.2,0.7-0.3,1.1-0.1c0.3,0.2,0.5,0.6,0.5,1c-0.2,2.7-1.5,5.1-3.6,6.8C16.6,21.2,14.4,22,12.1,22zM9.3,4.4c-2.9,1-5,3.6-5.2,6.8c-0.4,4.4,2.8,8.3,7.2,8.7c2.1,0.2,4.2-0.4,5.8-1.8c1.1-0.9,1.9-2.1,2.4-3.4c-2.5,0.9-5.3,0.5-7.5-1.1C9.2,11.4,8.1,7.7,9.3,4.4z"></path></svg><!--]--></span></span></button></div><div class="VPSocialLinks VPNavBarSocialLinks social-links" data-v-2298dee3 data-v-bb62c99d data-v-ab59833c><!--[--><a class="VPSocialLink no-icon" href="https://github.com/kaiyuanshe/2024-China-Open-Source-Report" aria-label="github" target="_blank" rel="noopener" data-v-ab59833c data-v-5d9347fb><svg role="img" viewBox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><title>GitHub</title><path d="M12 .297c-6.63 0-12 5.373-12 12 0 5.303 3.438 9.8 8.205 11.385.6.113.82-.258.82-.577 0-.285-.01-1.04-.015-2.04-3.338.724-4.042-1.61-4.042-1.61C4.422 18.07 3.633 17.7 3.633 17.7c-1.087-.744.084-.729.084-.729 1.205.084 1.838 1.236 1.838 1.236 1.07 1.835 2.809 1.305 3.495.998.108-.776.417-1.305.76-1.605-2.665-.3-5.466-1.332-5.466-5.93 0-1.31.465-2.38 1.235-3.22-.135-.303-.54-1.523.105-3.176 0 0 1.005-.322 3.3 1.23.96-.267 1.98-.399 3-.405 1.02.006 2.04.138 3 .405 2.28-1.552 3.285-1.23 3.285-1.23.645 1.653.24 2.873.12 3.176.765.84 1.23 1.91 1.23 3.22 0 4.61-2.805 5.625-5.475 5.92.42.36.81 1.096.81 2.22 0 1.606-.015 2.896-.015 3.286 0 .315.21.69.825.57C20.565 22.092 24 17.592 24 12.297c0-6.627-5.373-12-12-12"/></svg></a><a class="VPSocialLink no-icon" href="https://atomgit.com/kaiyuanshe/2024-China-Open-Source-Report" aria-label target="_blank" rel="noopener" data-v-ab59833c data-v-5d9347fb><?xml version="1.0" encoding="UTF-8"?><svg id="_图层_1" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" viewBox="0 0 248.7 143.1"><defs><style>.cls-1{fill:url(#linear-gradient);}.cls-1,.cls-2{fill-rule:evenodd;}.cls-2{fill:#251714;}</style><linearGradient id="linear-gradient" x1="135.9" y1="69.1" x2="112" y2="14.8" gradientUnits="userSpaceOnUse"><stop offset="0" stop-color="#0080ff"/><stop offset="1" stop-color="#00ebff"/></linearGradient></defs><path class="cls-1" d="m162.4,23.2c3,5.9,4.7,12.5,4.7,19.6s-.1,4-.4,6c-9.3-5.5-19.3-9-29.6-10.6l-6.2-14.9h31.5Zm-45.1,14.2c2.3-.2,4.7-.3,7-.3,2.4,0,4.7.1,7,.3l-3.9-12.3c-.5-1.4-1.8-2.1-3.1-2.1-1.3,0-2.6.7-3.1,2.1l-3.9,12.3h0Zm-31-14.2h31.5l-6.2,14.9c-10.3,1.6-20.3,5.2-29.6,10.6-.3-2-.4-4-.4-6,0-7,1.7-13.7,4.7-19.6h0Zm77.9,35.2c-7.5-7.9-16.2-13.2-25.3-16l12,28.9c1.1,2.6,2.5,2.9,4.2,1.2,3.9-4,7-8.8,9.1-14.1h0Zm-79.7,0c2.1,5.3,5.2,10.1,9.1,14.1,1.7,1.7,3.1,1.4,4.2-1.2l12-28.9c-9.1,2.8-17.8,8.1-25.3,16h0Zm60.6,21.8l-12.6-39.3c-2.7-.5-5.4-.7-8.2-.7-2.7,0-5.5.3-8.2.7l-12.6,39.3c6.2,3.4,13.2,5.4,20.8,5.4s14.6-1.9,20.8-5.4h0ZM124.3,0c13.7,0,25.8,6.4,33.7,16.4-10.9,2.6-22.2,3.9-33.7,3.9s-22.7-1.3-33.7-3.9C98.5,6.4,110.7,0,124.3,0Z"/><path class="cls-2" d="m27.6,102.9h5.2l-1.1-3.5h3.3l1.1,3.5h4.2v2.2h-7.2v2.7h6v14.3c0,.9-.7,1.6-1.6,1.6h-4.3v-2.2h2.6v-11.5h-2.8v1.5c0,4-.6,9.2-2.9,12.2h-3c2.2-4.1,2.6-8,2.6-11.9v-6.8h-2.2v-2.2h0Zm-26.7-3.1h22.8v2.2h-3.6v8.5h4.4v2.2h-4.4v11.1h-3.3v-11.1h-8.4c-.2,3.7-1.3,7.9-4.3,11.1H.4c3.1-3.2,4.5-7.2,4.7-11.1H0v-2.2h5.1v-8.5H.9v-2.2h0Zm16,2.2h-8.4v8.5h8.4v-8.5h0Zm213.8,0c-.8,2.8-2.1,5.2-3.8,7.2h-3.1c1.8-2.6,3.3-5.4,4.1-9.4h16.8c.8,4,2.3,6.7,4.1,9.4h-3.1c-1.8-2-3.1-4.4-3.8-7.2h-11.1Zm-.2,19.6h11.8l-1.6-4.3h3.4l2.4,6.5h-20.5l3.7-9.1h-3.9v-2.2h20.9v2.2h-13.4l-2.8,6.9h0Zm-1.5-14.6v2.2h14.5v-2.2h-14.5Zm-26.3-5c-.8,2.8-2.1,5.2-3.8,7.2h-3.1c1.8-2.6,3.3-5.4,4.1-9.4h16.8c.8,4,2.3,6.7,4.1,9.4h-3.1c-1.8-2-3.1-4.4-3.8-7.2h-11.1Zm14.9,14.6h-3.4l-1.7,5h-2.6v-6.9h8.8v-2.2h-8.8v-3.3h5.6v-2.2h-14.5v2.2h5.6v3.3h-8.8v2.2h8.8v6.9h-2.6l-1.7-5h-3.4l1.7,5h-4.6v2.2h24.5v-2.2h-4.6l1.7-5h0Zm-48.7-15.6h2.5v-1.6h3.3v1.6h11.2v-1.6h3.3v1.6h2.5v2.2h-2.5v9.3h3.4v2.2h-3.4c.6,1.8,1.4,3.4,2.8,5.3h-3.1c-1.4-1.4-2.4-3.4-3-5.3h-3.9v2h4v2.2h-4v2.6h10.6v2.2h-24.5v-2.2h10.6v-2.6h-4v-2.2h4v-2h-3.9c-.6,2-1.6,3.9-3,5.3h-3.1c1.4-1.9,2.3-3.5,2.8-5.3h-3.4v-2.2h3.4v-9.3h-2.5v-2.2h0Zm5.8,7.7h10.2v2.2h-10.2v1.7h11.2v-9.3h-11.2v1.7h10.2v2.2h-10.2v1.7h0Zm-28.1-8.8h18.6v2.2h-6.3l-.6,2.5h5.8v8.6c0,.9-.7,1.6-1.6,1.6h-3.2v7.4c0,.9-.7,1.6-1.6,1.6h-2.9v-2.2h1.3v-6.9h-4.8v-10.2h3.6l.6-2.5h-5.6v9.8c0,4-.5,8.9-2.3,12h-3c1.8-4,2-7.9,2-11.7v-12.3h0Zm-6.6,5.3l1.4,3.7h3.3l-1.4-3.7h-3.3Zm0-5.3l1.4,3.7h3.3l-1.4-3.7h-3.3Zm4.6,10.7h-3.3c0,5.4-.6,8.8-2.2,13.3h3.2c1.5-3.4,2.3-8,2.3-13.3h0Zm15.7,6c.1,2.6.7,5.4,2.1,7.3h3c-1.4-2.5-1.8-4.6-1.9-7.3h-3.2Zm-5.5,0h-3.2c-.1,2.7-.5,4.8-1.9,7.3h3c1.4-1.9,2-4.7,2.1-7.3h0Zm-.3-8h5.3v2.2h-5.3v1.8h6.3v-5.9h-6.3v1.8h0Zm-41.7-8.7h22.8v2.2h-3.6v8.5h4.4v2.2h-4.4v11.1h-3.3v-11.1h-8.4c-.2,3.7-1.3,7.9-4.3,11.1h-3.7c3.1-3.2,4.5-7.2,4.7-11.1h-5.1v-2.2h5.1v-8.5h-4.3v-2.2h0Zm16,2.2h-8.4v8.5h8.4v-8.5h0Zm-43.9-2.2h22.8v2.2l-8.8,6.8v1.7h9.6v2.2h-9.6v9.4c0,.9-.7,1.6-1.6,1.6h-8.1v-2.2h6.4v-8.9h-11.6v-2.2h11.6v-3.1l6.9-5.4h-17.7v-2.2h0Zm-27.2,0h23.3v2.2h-8.2l-.7,2.5h7.9v8.6c0,.9-.7,1.6-1.6,1.6h-5.3v7.4c0,.9-.7,1.6-1.6,1.6h-4v-2.2h2.3v-6.9h-6.9v-10.2h5.7l.7-2.5h-8.4v9.8c0,4-.5,8.9-2.3,12h-3c1.8-4,2-7.9,2-11.7v-12.3h0Zm17.8,16.6c.2,2.6.8,5.5,2.3,7.3h3.2c-1.5-2.5-2-4.6-2.2-7.3h-3.3Zm-8.5,0h-3.3c-.2,2.7-.7,4.8-2.2,7.3h3.2c1.5-1.9,2.1-4.7,2.3-7.3h0Zm-.8-8h9.2v2.2h-9.2v1.8h10.5v-5.9h-10.5v1.8h0Zm-17,15.3h4.3c-2.1-2-3.8-4-5-6,2.1-3.6,3.2-7.6,3.4-12.7h1.3v-2.2h-7.7c.2-1.2.3-2.4.3-3.5h-3.4c-.1,4-.6,6.8-2.3,10.7h2.1c.5,2.6,1.3,5.3,2.7,7.7-1.4,2.3-3.1,4.2-5,6h3.9c1.1-1,2-2.1,2.8-3.2.8,1.1,1.7,2.1,2.7,3.2h0Zm-4.6-17c.2-.5.3-1.1.4-1.6h3.5c-.3,3.5-.9,6.5-2,9.1-1-2.3-1.6-4.8-1.9-7.5h0ZM6,143.1c-1.2,0-2.2-.2-3.1-.7-.9-.5-1.6-1.1-2.1-2-.5-.9-.8-1.8-.8-2.9s.3-2.1.8-2.9c.5-.9,1.2-1.5,2.1-2,.9-.5,2-.7,3.1-.7s2.2.2,3.1.7,1.6,1.1,2.1,2c.5.9.8,1.8.8,2.9s-.3,2.1-.8,2.9c-.5.9-1.2,1.5-2.1,2-.9.5-2,.7-3.1.7h0Zm0-1.7c.7,0,1.3-.2,1.9-.5.5-.3,1-.8,1.3-1.4.3-.6.4-1.3.4-2.1s-.1-1.5-.4-2.1c-.3-.6-.7-1.1-1.3-1.4-.5-.3-1.2-.5-1.9-.5s-1.3.2-1.8.5c-.5.3-1,.8-1.3,1.4-.3.6-.4,1.3-.4,2.1s.1,1.5.4,2.1c.3.6.7,1.1,1.3,1.4.5.3,1.2.5,1.8.5h0Zm18.5-5.9c0,.7-.2,1.3-.5,1.9-.4.5-.9,1-1.5,1.3-.7.3-1.4.4-2.3.4h-2.3v3.8h-2.5v-10.9h4.8c.9,0,1.6.1,2.3.4.7.3,1.2.7,1.5,1.3.4.5.5,1.2.5,1.9h0Zm-2.5,0c0-.6-.2-1.1-.6-1.4-.4-.3-1-.5-1.8-.5h-1.8v3.9h1.8c.8,0,1.4-.2,1.8-.5.4-.3.6-.8.6-1.4h0Zm8.1-1.8v2.6h5.3v1.7h-5.3v3.1h5.6v1.7h-8.1v-10.9h8.1v1.7h-5.6Zm19.9-1.7v10.9h-2.3l-6.2-7.6v7.6h-2.3v-10.9h2.6l5.8,7.2v-7.2h2.2Zm11,8.1h-4.7l-1,2.8h-2.5l4.4-10.9h3.1l4.4,10.9h-2.7l-1-2.8h0Zm-.6-1.7l-1.7-4.7-1.7,4.7h3.4Zm15.4-4.7h-3.5v9.2h-2.5v-9.2h-3.5v-1.7h9.5v1.7h0Zm8.3,9.4c-1.2,0-2.2-.2-3.1-.7-.9-.5-1.6-1.1-2.1-2-.5-.9-.8-1.8-.8-2.9s.3-2.1.8-2.9c.5-.9,1.2-1.5,2.1-2,.9-.5,2-.7,3.1-.7s2.2.2,3.1.7c.9.5,1.6,1.1,2.1,2,.5.9.8,1.8.8,2.9s-.3,2.1-.8,2.9c-.5.9-1.2,1.5-2.1,2-.9.5-2,.7-3.1.7h0Zm0-1.7c.7,0,1.3-.2,1.8-.5.5-.3,1-.8,1.3-1.4.3-.6.4-1.3.4-2.1s-.1-1.5-.4-2.1c-.3-.6-.7-1.1-1.3-1.4-.5-.3-1.2-.5-1.8-.5s-1.3.2-1.8.5-1,.8-1.3,1.4c-.3.6-.4,1.3-.4,2.1s.1,1.5.4,2.1c.3.6.7,1.1,1.3,1.4.5.3,1.1.5,1.8.5h0Zm23.1,1.5h-2.5v-8.1l-3.2,8.1h-2.7l-3.2-8.1v8.1h-2.3v-10.9h3.5l3.3,8.6,3.3-8.6h3.6v10.9h0Zm15.6-9.2v2.7h5.1v1.7h-5.1v4.7h-2.5v-10.9h8v1.7h-5.5Zm14.4,9.4c-1.2,0-2.2-.2-3.1-.7-.9-.5-1.6-1.1-2.1-2-.5-.9-.8-1.8-.8-2.9s.3-2.1.8-2.9c.5-.9,1.2-1.5,2.1-2,.9-.5,2-.7,3.1-.7s2.2.2,3.1.7c.9.5,1.6,1.1,2.1,2,.5.9.8,1.8.8,2.9s-.3,2.1-.8,2.9c-.5.9-1.2,1.5-2.1,2-.9.5-2,.7-3.1.7h0Zm0-1.7c.7,0,1.3-.2,1.8-.5.5-.3,1-.8,1.3-1.4.3-.6.4-1.3.4-2.1s-.1-1.5-.4-2.1c-.3-.6-.7-1.1-1.3-1.4s-1.2-.5-1.8-.5-1.3.2-1.8.5c-.5.3-1,.8-1.3,1.4-.3.6-.4,1.3-.4,2.1s.1,1.5.4,2.1c.3.6.7,1.1,1.3,1.4.5.3,1.2.5,1.8.5h0Zm14.6,1.7c-1.1,0-2-.2-2.8-.5-.8-.3-1.4-.8-1.9-1.4-.4-.6-.7-1.4-.7-2.2v-7h2.3v6.8c0,.9.3,1.5.8,1.9.5.4,1.2.7,2.2.7s1.7-.2,2.2-.7c.5-.4.7-1.1.7-1.9v-6.8h2.5v7c0,.8-.2,1.6-.7,2.2-.4.6-1.1,1.1-1.9,1.4-.8.3-1.7.5-2.8.5h0Zm19.7-11.1v10.9h-2.3l-6.2-7.6v7.6h-2.3v-10.9h2.6l5.8,7.2v-7.2h2.2Zm14.6,5.5c0,1.1-.2,2.1-.7,2.9-.5.8-1.2,1.5-2,1.9-.9.4-1.9.7-3.1.7h-5.1v-10.9h5.1c1.2,0,2.2.2,3.1.7.9.5,1.6,1.1,2,1.9.5.8.7,1.8.7,2.9h0Zm-2.5,0c0-1.1-.3-2-1-2.7-.7-.7-1.6-1-2.8-1h-2.1v7.5h2.1c1.2,0,2.2-.3,2.8-1,.7-.7,1-1.6,1-2.7h0Zm12.8,2.7h-4.7l-1,2.8h-2.5l4.4-10.9h3.1l4.4,10.9h-2.7l-1-2.8h0Zm-.6-1.7l-1.7-4.7-1.7,4.7h3.4Zm15.4-4.7h-3.5v9.2h-2.5v-9.2h-3.5v-1.7h9.5v1.7h0Zm5.7-1.7v10.9h-2.5v-10.9h2.5Zm9.5,11.1c-1.2,0-2.2-.2-3.1-.7-.9-.5-1.6-1.1-2.1-2-.5-.9-.8-1.8-.8-2.9s.3-2.1.8-2.9c.5-.9,1.2-1.5,2.1-2,.9-.5,2-.7,3.1-.7s2.2.2,3.1.7c.9.5,1.6,1.1,2.1,2,.5.9.8,1.8.8,2.9s-.3,2.1-.8,2.9c-.5.9-1.2,1.5-2.1,2-.9.5-2,.7-3.1.7h0Zm0-1.7c.7,0,1.3-.2,1.8-.5.5-.3,1-.8,1.3-1.4.3-.6.4-1.3.4-2.1s-.1-1.5-.4-2.1c-.3-.6-.7-1.1-1.3-1.4-.5-.3-1.2-.5-1.8-.5s-1.3.2-1.8.5c-.5.3-1,.8-1.3,1.4-.3.6-.4,1.3-.4,2.1s.2,1.5.4,2.1c.3.6.7,1.1,1.3,1.4.5.3,1.2.5,1.8.5h0Zm20.1-9.4v10.9h-2.3l-6.2-7.6v7.6h-2.3v-10.9h2.6l5.8,7.2v-7.2h2.2Z"/></svg></a><!--]--></div><div class="VPFlyout VPNavBarExtra extra" data-v-2298dee3 data-v-d621c6c4 data-v-7946f491><button type="button" class="button" aria-haspopup="true" aria-expanded="false" aria-label="extra navigation" data-v-7946f491><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" viewbox="0 0 24 24" class="icon" data-v-7946f491><circle cx="12" cy="12" r="2"></circle><circle cx="19" cy="12" r="2"></circle><circle cx="5" cy="12" r="2"></circle></svg></button><div class="menu" data-v-7946f491><div class="VPMenu" data-v-7946f491 data-v-93f7c741><!----><!--[--><!--[--><div class="group translations" data-v-d621c6c4><p class="trans-title" data-v-d621c6c4>English</p><!--[--><div class="VPMenuLink" data-v-d621c6c4 data-v-8775611d><a class="VPLink link" href="/2024-China-Open-Source-Report/ossAI.html" data-v-8775611d><!--[-->简体中文<!--]--></a></div><!--]--></div><div class="group" data-v-d621c6c4><div class="item appearance" data-v-d621c6c4><p class="label" data-v-d621c6c4>Appearance</p><div class="appearance-action" data-v-d621c6c4><button class="VPSwitch VPSwitchAppearance" type="button" role="switch" title="Switch to dark theme" aria-checked="false" data-v-d621c6c4 data-v-8457bf22 data-v-ade1a38d><span class="check" data-v-ade1a38d><span class="icon" data-v-ade1a38d><!--[--><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" viewbox="0 0 24 24" class="sun" data-v-8457bf22><path d="M12,18c-3.3,0-6-2.7-6-6s2.7-6,6-6s6,2.7,6,6S15.3,18,12,18zM12,8c-2.2,0-4,1.8-4,4c0,2.2,1.8,4,4,4c2.2,0,4-1.8,4-4C16,9.8,14.2,8,12,8z"></path><path d="M12,4c-0.6,0-1-0.4-1-1V1c0-0.6,0.4-1,1-1s1,0.4,1,1v2C13,3.6,12.6,4,12,4z"></path><path d="M12,24c-0.6,0-1-0.4-1-1v-2c0-0.6,0.4-1,1-1s1,0.4,1,1v2C13,23.6,12.6,24,12,24z"></path><path d="M5.6,6.6c-0.3,0-0.5-0.1-0.7-0.3L3.5,4.9c-0.4-0.4-0.4-1,0-1.4s1-0.4,1.4,0l1.4,1.4c0.4,0.4,0.4,1,0,1.4C6.2,6.5,5.9,6.6,5.6,6.6z"></path><path d="M19.8,20.8c-0.3,0-0.5-0.1-0.7-0.3l-1.4-1.4c-0.4-0.4-0.4-1,0-1.4s1-0.4,1.4,0l1.4,1.4c0.4,0.4,0.4,1,0,1.4C20.3,20.7,20,20.8,19.8,20.8z"></path><path d="M3,13H1c-0.6,0-1-0.4-1-1s0.4-1,1-1h2c0.6,0,1,0.4,1,1S3.6,13,3,13z"></path><path d="M23,13h-2c-0.6,0-1-0.4-1-1s0.4-1,1-1h2c0.6,0,1,0.4,1,1S23.6,13,23,13z"></path><path d="M4.2,20.8c-0.3,0-0.5-0.1-0.7-0.3c-0.4-0.4-0.4-1,0-1.4l1.4-1.4c0.4-0.4,1-0.4,1.4,0s0.4,1,0,1.4l-1.4,1.4C4.7,20.7,4.5,20.8,4.2,20.8z"></path><path d="M18.4,6.6c-0.3,0-0.5-0.1-0.7-0.3c-0.4-0.4-0.4-1,0-1.4l1.4-1.4c0.4-0.4,1-0.4,1.4,0s0.4,1,0,1.4l-1.4,1.4C18.9,6.5,18.6,6.6,18.4,6.6z"></path></svg><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" viewbox="0 0 24 24" class="moon" data-v-8457bf22><path d="M12.1,22c-0.3,0-0.6,0-0.9,0c-5.5-0.5-9.5-5.4-9-10.9c0.4-4.8,4.2-8.6,9-9c0.4,0,0.8,0.2,1,0.5c0.2,0.3,0.2,0.8-0.1,1.1c-2,2.7-1.4,6.4,1.3,8.4c2.1,1.6,5,1.6,7.1,0c0.3-0.2,0.7-0.3,1.1-0.1c0.3,0.2,0.5,0.6,0.5,1c-0.2,2.7-1.5,5.1-3.6,6.8C16.6,21.2,14.4,22,12.1,22zM9.3,4.4c-2.9,1-5,3.6-5.2,6.8c-0.4,4.4,2.8,8.3,7.2,8.7c2.1,0.2,4.2-0.4,5.8-1.8c1.1-0.9,1.9-2.1,2.4-3.4c-2.5,0.9-5.3,0.5-7.5-1.1C9.2,11.4,8.1,7.7,9.3,4.4z"></path></svg><!--]--></span></span></button></div></div></div><div class="group" data-v-d621c6c4><div class="item social-links" data-v-d621c6c4><div class="VPSocialLinks social-links-list" data-v-d621c6c4 data-v-ab59833c><!--[--><a class="VPSocialLink no-icon" href="https://github.com/kaiyuanshe/2024-China-Open-Source-Report" aria-label="github" target="_blank" rel="noopener" data-v-ab59833c data-v-5d9347fb><svg role="img" viewBox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><title>GitHub</title><path d="M12 .297c-6.63 0-12 5.373-12 12 0 5.303 3.438 9.8 8.205 11.385.6.113.82-.258.82-.577 0-.285-.01-1.04-.015-2.04-3.338.724-4.042-1.61-4.042-1.61C4.422 18.07 3.633 17.7 3.633 17.7c-1.087-.744.084-.729.084-.729 1.205.084 1.838 1.236 1.838 1.236 1.07 1.835 2.809 1.305 3.495.998.108-.776.417-1.305.76-1.605-2.665-.3-5.466-1.332-5.466-5.93 0-1.31.465-2.38 1.235-3.22-.135-.303-.54-1.523.105-3.176 0 0 1.005-.322 3.3 1.23.96-.267 1.98-.399 3-.405 1.02.006 2.04.138 3 .405 2.28-1.552 3.285-1.23 3.285-1.23.645 1.653.24 2.873.12 3.176.765.84 1.23 1.91 1.23 3.22 0 4.61-2.805 5.625-5.475 5.92.42.36.81 1.096.81 2.22 0 1.606-.015 2.896-.015 3.286 0 .315.21.69.825.57C20.565 22.092 24 17.592 24 12.297c0-6.627-5.373-12-12-12"/></svg></a><a class="VPSocialLink no-icon" href="https://atomgit.com/kaiyuanshe/2024-China-Open-Source-Report" aria-label target="_blank" rel="noopener" data-v-ab59833c data-v-5d9347fb><?xml version="1.0" encoding="UTF-8"?><svg id="_图层_1" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" viewBox="0 0 248.7 143.1"><defs><style>.cls-1{fill:url(#linear-gradient);}.cls-1,.cls-2{fill-rule:evenodd;}.cls-2{fill:#251714;}</style><linearGradient id="linear-gradient" x1="135.9" y1="69.1" x2="112" y2="14.8" gradientUnits="userSpaceOnUse"><stop offset="0" stop-color="#0080ff"/><stop offset="1" stop-color="#00ebff"/></linearGradient></defs><path class="cls-1" d="m162.4,23.2c3,5.9,4.7,12.5,4.7,19.6s-.1,4-.4,6c-9.3-5.5-19.3-9-29.6-10.6l-6.2-14.9h31.5Zm-45.1,14.2c2.3-.2,4.7-.3,7-.3,2.4,0,4.7.1,7,.3l-3.9-12.3c-.5-1.4-1.8-2.1-3.1-2.1-1.3,0-2.6.7-3.1,2.1l-3.9,12.3h0Zm-31-14.2h31.5l-6.2,14.9c-10.3,1.6-20.3,5.2-29.6,10.6-.3-2-.4-4-.4-6,0-7,1.7-13.7,4.7-19.6h0Zm77.9,35.2c-7.5-7.9-16.2-13.2-25.3-16l12,28.9c1.1,2.6,2.5,2.9,4.2,1.2,3.9-4,7-8.8,9.1-14.1h0Zm-79.7,0c2.1,5.3,5.2,10.1,9.1,14.1,1.7,1.7,3.1,1.4,4.2-1.2l12-28.9c-9.1,2.8-17.8,8.1-25.3,16h0Zm60.6,21.8l-12.6-39.3c-2.7-.5-5.4-.7-8.2-.7-2.7,0-5.5.3-8.2.7l-12.6,39.3c6.2,3.4,13.2,5.4,20.8,5.4s14.6-1.9,20.8-5.4h0ZM124.3,0c13.7,0,25.8,6.4,33.7,16.4-10.9,2.6-22.2,3.9-33.7,3.9s-22.7-1.3-33.7-3.9C98.5,6.4,110.7,0,124.3,0Z"/><path class="cls-2" d="m27.6,102.9h5.2l-1.1-3.5h3.3l1.1,3.5h4.2v2.2h-7.2v2.7h6v14.3c0,.9-.7,1.6-1.6,1.6h-4.3v-2.2h2.6v-11.5h-2.8v1.5c0,4-.6,9.2-2.9,12.2h-3c2.2-4.1,2.6-8,2.6-11.9v-6.8h-2.2v-2.2h0Zm-26.7-3.1h22.8v2.2h-3.6v8.5h4.4v2.2h-4.4v11.1h-3.3v-11.1h-8.4c-.2,3.7-1.3,7.9-4.3,11.1H.4c3.1-3.2,4.5-7.2,4.7-11.1H0v-2.2h5.1v-8.5H.9v-2.2h0Zm16,2.2h-8.4v8.5h8.4v-8.5h0Zm213.8,0c-.8,2.8-2.1,5.2-3.8,7.2h-3.1c1.8-2.6,3.3-5.4,4.1-9.4h16.8c.8,4,2.3,6.7,4.1,9.4h-3.1c-1.8-2-3.1-4.4-3.8-7.2h-11.1Zm-.2,19.6h11.8l-1.6-4.3h3.4l2.4,6.5h-20.5l3.7-9.1h-3.9v-2.2h20.9v2.2h-13.4l-2.8,6.9h0Zm-1.5-14.6v2.2h14.5v-2.2h-14.5Zm-26.3-5c-.8,2.8-2.1,5.2-3.8,7.2h-3.1c1.8-2.6,3.3-5.4,4.1-9.4h16.8c.8,4,2.3,6.7,4.1,9.4h-3.1c-1.8-2-3.1-4.4-3.8-7.2h-11.1Zm14.9,14.6h-3.4l-1.7,5h-2.6v-6.9h8.8v-2.2h-8.8v-3.3h5.6v-2.2h-14.5v2.2h5.6v3.3h-8.8v2.2h8.8v6.9h-2.6l-1.7-5h-3.4l1.7,5h-4.6v2.2h24.5v-2.2h-4.6l1.7-5h0Zm-48.7-15.6h2.5v-1.6h3.3v1.6h11.2v-1.6h3.3v1.6h2.5v2.2h-2.5v9.3h3.4v2.2h-3.4c.6,1.8,1.4,3.4,2.8,5.3h-3.1c-1.4-1.4-2.4-3.4-3-5.3h-3.9v2h4v2.2h-4v2.6h10.6v2.2h-24.5v-2.2h10.6v-2.6h-4v-2.2h4v-2h-3.9c-.6,2-1.6,3.9-3,5.3h-3.1c1.4-1.9,2.3-3.5,2.8-5.3h-3.4v-2.2h3.4v-9.3h-2.5v-2.2h0Zm5.8,7.7h10.2v2.2h-10.2v1.7h11.2v-9.3h-11.2v1.7h10.2v2.2h-10.2v1.7h0Zm-28.1-8.8h18.6v2.2h-6.3l-.6,2.5h5.8v8.6c0,.9-.7,1.6-1.6,1.6h-3.2v7.4c0,.9-.7,1.6-1.6,1.6h-2.9v-2.2h1.3v-6.9h-4.8v-10.2h3.6l.6-2.5h-5.6v9.8c0,4-.5,8.9-2.3,12h-3c1.8-4,2-7.9,2-11.7v-12.3h0Zm-6.6,5.3l1.4,3.7h3.3l-1.4-3.7h-3.3Zm0-5.3l1.4,3.7h3.3l-1.4-3.7h-3.3Zm4.6,10.7h-3.3c0,5.4-.6,8.8-2.2,13.3h3.2c1.5-3.4,2.3-8,2.3-13.3h0Zm15.7,6c.1,2.6.7,5.4,2.1,7.3h3c-1.4-2.5-1.8-4.6-1.9-7.3h-3.2Zm-5.5,0h-3.2c-.1,2.7-.5,4.8-1.9,7.3h3c1.4-1.9,2-4.7,2.1-7.3h0Zm-.3-8h5.3v2.2h-5.3v1.8h6.3v-5.9h-6.3v1.8h0Zm-41.7-8.7h22.8v2.2h-3.6v8.5h4.4v2.2h-4.4v11.1h-3.3v-11.1h-8.4c-.2,3.7-1.3,7.9-4.3,11.1h-3.7c3.1-3.2,4.5-7.2,4.7-11.1h-5.1v-2.2h5.1v-8.5h-4.3v-2.2h0Zm16,2.2h-8.4v8.5h8.4v-8.5h0Zm-43.9-2.2h22.8v2.2l-8.8,6.8v1.7h9.6v2.2h-9.6v9.4c0,.9-.7,1.6-1.6,1.6h-8.1v-2.2h6.4v-8.9h-11.6v-2.2h11.6v-3.1l6.9-5.4h-17.7v-2.2h0Zm-27.2,0h23.3v2.2h-8.2l-.7,2.5h7.9v8.6c0,.9-.7,1.6-1.6,1.6h-5.3v7.4c0,.9-.7,1.6-1.6,1.6h-4v-2.2h2.3v-6.9h-6.9v-10.2h5.7l.7-2.5h-8.4v9.8c0,4-.5,8.9-2.3,12h-3c1.8-4,2-7.9,2-11.7v-12.3h0Zm17.8,16.6c.2,2.6.8,5.5,2.3,7.3h3.2c-1.5-2.5-2-4.6-2.2-7.3h-3.3Zm-8.5,0h-3.3c-.2,2.7-.7,4.8-2.2,7.3h3.2c1.5-1.9,2.1-4.7,2.3-7.3h0Zm-.8-8h9.2v2.2h-9.2v1.8h10.5v-5.9h-10.5v1.8h0Zm-17,15.3h4.3c-2.1-2-3.8-4-5-6,2.1-3.6,3.2-7.6,3.4-12.7h1.3v-2.2h-7.7c.2-1.2.3-2.4.3-3.5h-3.4c-.1,4-.6,6.8-2.3,10.7h2.1c.5,2.6,1.3,5.3,2.7,7.7-1.4,2.3-3.1,4.2-5,6h3.9c1.1-1,2-2.1,2.8-3.2.8,1.1,1.7,2.1,2.7,3.2h0Zm-4.6-17c.2-.5.3-1.1.4-1.6h3.5c-.3,3.5-.9,6.5-2,9.1-1-2.3-1.6-4.8-1.9-7.5h0ZM6,143.1c-1.2,0-2.2-.2-3.1-.7-.9-.5-1.6-1.1-2.1-2-.5-.9-.8-1.8-.8-2.9s.3-2.1.8-2.9c.5-.9,1.2-1.5,2.1-2,.9-.5,2-.7,3.1-.7s2.2.2,3.1.7,1.6,1.1,2.1,2c.5.9.8,1.8.8,2.9s-.3,2.1-.8,2.9c-.5.9-1.2,1.5-2.1,2-.9.5-2,.7-3.1.7h0Zm0-1.7c.7,0,1.3-.2,1.9-.5.5-.3,1-.8,1.3-1.4.3-.6.4-1.3.4-2.1s-.1-1.5-.4-2.1c-.3-.6-.7-1.1-1.3-1.4-.5-.3-1.2-.5-1.9-.5s-1.3.2-1.8.5c-.5.3-1,.8-1.3,1.4-.3.6-.4,1.3-.4,2.1s.1,1.5.4,2.1c.3.6.7,1.1,1.3,1.4.5.3,1.2.5,1.8.5h0Zm18.5-5.9c0,.7-.2,1.3-.5,1.9-.4.5-.9,1-1.5,1.3-.7.3-1.4.4-2.3.4h-2.3v3.8h-2.5v-10.9h4.8c.9,0,1.6.1,2.3.4.7.3,1.2.7,1.5,1.3.4.5.5,1.2.5,1.9h0Zm-2.5,0c0-.6-.2-1.1-.6-1.4-.4-.3-1-.5-1.8-.5h-1.8v3.9h1.8c.8,0,1.4-.2,1.8-.5.4-.3.6-.8.6-1.4h0Zm8.1-1.8v2.6h5.3v1.7h-5.3v3.1h5.6v1.7h-8.1v-10.9h8.1v1.7h-5.6Zm19.9-1.7v10.9h-2.3l-6.2-7.6v7.6h-2.3v-10.9h2.6l5.8,7.2v-7.2h2.2Zm11,8.1h-4.7l-1,2.8h-2.5l4.4-10.9h3.1l4.4,10.9h-2.7l-1-2.8h0Zm-.6-1.7l-1.7-4.7-1.7,4.7h3.4Zm15.4-4.7h-3.5v9.2h-2.5v-9.2h-3.5v-1.7h9.5v1.7h0Zm8.3,9.4c-1.2,0-2.2-.2-3.1-.7-.9-.5-1.6-1.1-2.1-2-.5-.9-.8-1.8-.8-2.9s.3-2.1.8-2.9c.5-.9,1.2-1.5,2.1-2,.9-.5,2-.7,3.1-.7s2.2.2,3.1.7c.9.5,1.6,1.1,2.1,2,.5.9.8,1.8.8,2.9s-.3,2.1-.8,2.9c-.5.9-1.2,1.5-2.1,2-.9.5-2,.7-3.1.7h0Zm0-1.7c.7,0,1.3-.2,1.8-.5.5-.3,1-.8,1.3-1.4.3-.6.4-1.3.4-2.1s-.1-1.5-.4-2.1c-.3-.6-.7-1.1-1.3-1.4-.5-.3-1.2-.5-1.8-.5s-1.3.2-1.8.5-1,.8-1.3,1.4c-.3.6-.4,1.3-.4,2.1s.1,1.5.4,2.1c.3.6.7,1.1,1.3,1.4.5.3,1.1.5,1.8.5h0Zm23.1,1.5h-2.5v-8.1l-3.2,8.1h-2.7l-3.2-8.1v8.1h-2.3v-10.9h3.5l3.3,8.6,3.3-8.6h3.6v10.9h0Zm15.6-9.2v2.7h5.1v1.7h-5.1v4.7h-2.5v-10.9h8v1.7h-5.5Zm14.4,9.4c-1.2,0-2.2-.2-3.1-.7-.9-.5-1.6-1.1-2.1-2-.5-.9-.8-1.8-.8-2.9s.3-2.1.8-2.9c.5-.9,1.2-1.5,2.1-2,.9-.5,2-.7,3.1-.7s2.2.2,3.1.7c.9.5,1.6,1.1,2.1,2,.5.9.8,1.8.8,2.9s-.3,2.1-.8,2.9c-.5.9-1.2,1.5-2.1,2-.9.5-2,.7-3.1.7h0Zm0-1.7c.7,0,1.3-.2,1.8-.5.5-.3,1-.8,1.3-1.4.3-.6.4-1.3.4-2.1s-.1-1.5-.4-2.1c-.3-.6-.7-1.1-1.3-1.4s-1.2-.5-1.8-.5-1.3.2-1.8.5c-.5.3-1,.8-1.3,1.4-.3.6-.4,1.3-.4,2.1s.1,1.5.4,2.1c.3.6.7,1.1,1.3,1.4.5.3,1.2.5,1.8.5h0Zm14.6,1.7c-1.1,0-2-.2-2.8-.5-.8-.3-1.4-.8-1.9-1.4-.4-.6-.7-1.4-.7-2.2v-7h2.3v6.8c0,.9.3,1.5.8,1.9.5.4,1.2.7,2.2.7s1.7-.2,2.2-.7c.5-.4.7-1.1.7-1.9v-6.8h2.5v7c0,.8-.2,1.6-.7,2.2-.4.6-1.1,1.1-1.9,1.4-.8.3-1.7.5-2.8.5h0Zm19.7-11.1v10.9h-2.3l-6.2-7.6v7.6h-2.3v-10.9h2.6l5.8,7.2v-7.2h2.2Zm14.6,5.5c0,1.1-.2,2.1-.7,2.9-.5.8-1.2,1.5-2,1.9-.9.4-1.9.7-3.1.7h-5.1v-10.9h5.1c1.2,0,2.2.2,3.1.7.9.5,1.6,1.1,2,1.9.5.8.7,1.8.7,2.9h0Zm-2.5,0c0-1.1-.3-2-1-2.7-.7-.7-1.6-1-2.8-1h-2.1v7.5h2.1c1.2,0,2.2-.3,2.8-1,.7-.7,1-1.6,1-2.7h0Zm12.8,2.7h-4.7l-1,2.8h-2.5l4.4-10.9h3.1l4.4,10.9h-2.7l-1-2.8h0Zm-.6-1.7l-1.7-4.7-1.7,4.7h3.4Zm15.4-4.7h-3.5v9.2h-2.5v-9.2h-3.5v-1.7h9.5v1.7h0Zm5.7-1.7v10.9h-2.5v-10.9h2.5Zm9.5,11.1c-1.2,0-2.2-.2-3.1-.7-.9-.5-1.6-1.1-2.1-2-.5-.9-.8-1.8-.8-2.9s.3-2.1.8-2.9c.5-.9,1.2-1.5,2.1-2,.9-.5,2-.7,3.1-.7s2.2.2,3.1.7c.9.5,1.6,1.1,2.1,2,.5.9.8,1.8.8,2.9s-.3,2.1-.8,2.9c-.5.9-1.2,1.5-2.1,2-.9.5-2,.7-3.1.7h0Zm0-1.7c.7,0,1.3-.2,1.8-.5.5-.3,1-.8,1.3-1.4.3-.6.4-1.3.4-2.1s-.1-1.5-.4-2.1c-.3-.6-.7-1.1-1.3-1.4-.5-.3-1.2-.5-1.8-.5s-1.3.2-1.8.5c-.5.3-1,.8-1.3,1.4-.3.6-.4,1.3-.4,2.1s.2,1.5.4,2.1c.3.6.7,1.1,1.3,1.4.5.3,1.2.5,1.8.5h0Zm20.1-9.4v10.9h-2.3l-6.2-7.6v7.6h-2.3v-10.9h2.6l5.8,7.2v-7.2h2.2Z"/></svg></a><!--]--></div></div></div><!--]--><!--]--></div></div></div><!--[--><!--]--><button type="button" class="VPNavBarHamburger hamburger" aria-label="mobile navigation" aria-expanded="false" aria-controls="VPNavScreen" data-v-2298dee3 data-v-8e7e5700><span class="container" data-v-8e7e5700><span class="top" data-v-8e7e5700></span><span class="middle" data-v-8e7e5700></span><span class="bottom" data-v-8e7e5700></span></span></button></div></div></div></div><div class="divider" data-v-2298dee3><div class="divider-line" data-v-2298dee3></div></div></div><!----></header><div class="VPLocalNav has-sidebar empty" data-v-4709876f data-v-bb531a0e><div class="container" data-v-bb531a0e><button class="menu" aria-expanded="false" aria-controls="VPSidebarNav" data-v-bb531a0e><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" viewbox="0 0 24 24" class="menu-icon" data-v-bb531a0e><path d="M17,11H3c-0.6,0-1-0.4-1-1s0.4-1,1-1h14c0.6,0,1,0.4,1,1S17.6,11,17,11z"></path><path d="M21,7H3C2.4,7,2,6.6,2,6s0.4-1,1-1h18c0.6,0,1,0.4,1,1S21.6,7,21,7z"></path><path d="M21,15H3c-0.6,0-1-0.4-1-1s0.4-1,1-1h18c0.6,0,1,0.4,1,1S21.6,15,21,15z"></path><path d="M17,19H3c-0.6,0-1-0.4-1-1s0.4-1,1-1h14c0.6,0,1,0.4,1,1S17.6,19,17,19z"></path></svg><span class="menu-text" data-v-bb531a0e>Menu</span></button><div class="VPLocalNavOutlineDropdown" style="--vp-vh:0px;" data-v-bb531a0e data-v-8cc2150f><button data-v-8cc2150f>Return to top</button><!----></div></div></div><aside class="VPSidebar" data-v-4709876f data-v-10a16eb0><div class="curtain" data-v-10a16eb0></div><nav class="nav" id="VPSidebarNav" aria-labelledby="sidebar-aria-label" tabindex="-1" data-v-10a16eb0><span class="visually-hidden" id="sidebar-aria-label" data-v-10a16eb0> Sidebar Navigation </span><!--[--><!--]--><!--[--><div class="group" data-v-10a16eb0><section class="VPSidebarItem level-0 has-active" data-v-10a16eb0 data-v-28493fdc><!----><div class="items" data-v-28493fdc><!--[--><div class="VPSidebarItem level-1 is-link" data-v-28493fdc data-v-28493fdc><div class="item" data-v-28493fdc><div class="indicator" data-v-28493fdc></div><a class="VPLink link link" href="/2024-China-Open-Source-Report/en/preface.html" data-v-28493fdc><!--[--><p class="text" data-v-28493fdc>Preface</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-28493fdc data-v-28493fdc><div class="item" data-v-28493fdc><div class="indicator" data-v-28493fdc></div><a class="VPLink link link" href="/2024-China-Open-Source-Report/en/questionnaire.html" data-v-28493fdc><!--[--><p class="text" data-v-28493fdc>OSS Questionnaire</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-28493fdc data-v-28493fdc><div class="item" data-v-28493fdc><div class="indicator" data-v-28493fdc></div><a class="VPLink link link" href="/2024-China-Open-Source-Report/en/data.html" data-v-28493fdc><!--[--><p class="text" data-v-28493fdc>OSS Data Analytics</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-28493fdc data-v-28493fdc><div class="item" data-v-28493fdc><div class="indicator" data-v-28493fdc></div><a class="VPLink link link" href="/2024-China-Open-Source-Report/en/commercialization.html" data-v-28493fdc><!--[--><p class="text" data-v-28493fdc>OSS Commercialization</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-28493fdc data-v-28493fdc><div class="item" data-v-28493fdc><div class="indicator" data-v-28493fdc></div><a class="VPLink link link" href="/2024-China-Open-Source-Report/en/ossAI.html" data-v-28493fdc><!--[--><p class="text" data-v-28493fdc>OSS AI</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-28493fdc data-v-28493fdc><div class="item" data-v-28493fdc><div class="indicator" data-v-28493fdc></div><a class="VPLink link link" href="/2024-China-Open-Source-Report/en/open-source-milestones.html" data-v-28493fdc><!--[--><p class="text" data-v-28493fdc>OSS Chronicle</p><!--]--></a><!----></div><!----></div><!--]--></div></section></div><!--]--><!--[--><!--]--></nav></aside><div class="VPContent has-sidebar" id="VPContent" data-v-4709876f data-v-c85ac790><div class="VPDoc has-sidebar has-aside" data-v-c85ac790 data-v-337c2e06><!--[--><!--]--><div class="container" data-v-337c2e06><div class="aside" data-v-337c2e06><div class="aside-curtain" data-v-337c2e06></div><div class="aside-container" data-v-337c2e06><div class="aside-content" data-v-337c2e06><div class="VPDocAside" data-v-337c2e06 data-v-89c525ad><!--[--><!--]--><!--[--><!--]--><div class="VPDocAsideOutline" role="navigation" data-v-89c525ad data-v-fc8ab626><div class="content" data-v-fc8ab626><div class="outline-marker" data-v-fc8ab626></div><div class="outline-title" role="heading" aria-level="2" data-v-fc8ab626>On this page</div><nav aria-labelledby="doc-outline-aria-label" data-v-fc8ab626><span class="visually-hidden" id="doc-outline-aria-label" data-v-fc8ab626> Table of Contents for current page </span><ul class="VPDocOutlineItem root" data-v-fc8ab626 data-v-745513a7><!--[--><!--]--></ul></nav></div></div><!--[--><!--]--><div class="spacer" data-v-89c525ad></div><!--[--><!--]--><!----><!--[--><!--]--><!--[--><!--]--></div></div></div></div><div class="content" data-v-337c2e06><div class="content-container" data-v-337c2e06><!--[--><!--]--><main class="main" data-v-337c2e06><div style="position:relative;" class="vp-doc _2024-China-Open-Source-Report_en_ossAI external-link-icon-enabled" data-v-337c2e06><div><h1 id="open-source-ai" tabindex="-1">Open Source AI <a class="header-anchor" href="#open-source-ai" aria-label="Permalink to &quot;Open Source AI&quot;">​</a></h1><p>Author: Biaowei Zhuang &amp; Ted Liu, Translator &amp; Reviewer: Ted Liu</p><h2 id="_1-overview" tabindex="-1">1. Overview <a class="header-anchor" href="#_1-overview" aria-label="Permalink to &quot;1. Overview&quot;">​</a></h2><p>Since the groundbreaking emergence of ChatGPT at the end of 2022, 2023 has been a pivotal year for the development of artificial intelligence. By 2024, the explosive growth of open-source AI has completely rewritten industry rules—shifting from technological monopolies to collective innovation, from lab-driven research to real-world industrial applications. An AI paradigm shift, led by open source, has officially arrived. As a result, the <strong>2024 China Open Source Annual Report</strong> has decided to dedicate a separate section, <strong>&#39;Open Source AI&#39;</strong>, to provide readers with a more comprehensive perspective on this transformative movement.</p><p>On July 27, 2024, Meta CEO Mark Zuckerberg published a groundbreaking article titled <strong>&quot;AI is approaching an open-source inflection point.&quot;</strong> He emphasized the profound impact of open-source AI on the entire industry, stating that as AI technology advances and community collaboration strengthens, open source will be the key driver of innovation and the widespread adoption of AI. The key points in his article provide a strong summary of the current state and development trends of open-source AI:</p><ul><li><strong>The Rise of Open-Source AI</strong> An increasing number of companies and research institutions are releasing open-source AI models and tools, a trend that is transforming the industry&#39;s competitive landscape. Open source is driving technological democratization, allowing more developers and enterprises to access and utilize cutting-edge AI technology.</li><li><strong>Balancing Business and Technology</strong> Enterprises leverage open-source AI to attract developer ecosystems while integrating proprietary and open-source technologies to build unique competitive advantages. This dual-track strategy allows companies to benefit from open source while maintaining the competitiveness of their core technologies.</li><li><strong>The Importance of Community and Collaboration</strong> The key to the success of open-source AI lies in an active developer community and a culture of collaboration. These communities provide strong support for model improvements, bug fixes, and new feature development, significantly accelerating the pace of AI innovation.</li><li><strong>Ethics and Transparency</strong> Open-source AI introduces new possibilities for transparency and accountability in AI technology. Open-source code makes the training and decision-making processes of models more transparent, helping to address bias and ethical concerns.</li><li><strong>Future Challenges</strong> Despite its vast potential, open-source AI faces several challenges, including copyright protection, data privacy, security vulnerabilities, and commercial sustainability. The industry must find solutions to these issues to fully unlock the potential of open-source AI.</li></ul><p>As Zuckerberg asserted, <strong>&quot;Open Source AI is the Path Forward.&quot;</strong> In 2024, open-source models like Llama and Mistral are rapidly approaching—and even surpassing—proprietary benchmarks. Chinese teams such as DeepSeek, Tongyi Qianwen (Qwen), and Zhipu are reshaping the global landscape with their <strong>&quot;radical openness&quot;</strong> and <strong>&quot;cost revolution.&quot;</strong> Platforms like Hugging Face and GitHub are bringing together hundreds of thousands of developers, making collaboration and knowledge-sharing the cornerstone of technological democratization. Open source is not just about making code accessible—it represents a <strong>redefinition of technological beliefs.</strong> It transforms innovation from being confined to research labs to reaching everyday life, shifting AI from corporate monopolies to a movement of collective creation.</p><p>However, the rapid surge in technology also brings <strong>unprecedented challenges.</strong> Behind the increasing convergence in performance lies the failure of benchmark testing and growing concerns over data contamination. The <strong>low cost and high availability</strong> of open-source models have triggered an intense global AI price war. NVIDIA’s <strong>chip dominance</strong> and the <strong>EU’s strict regulations</strong> reflect the complex entanglement between technology and power. Meanwhile, the <strong>misuse of synthetic data</strong> and the <strong>proliferation of deepfakes</strong> continuously raise alarms over ethics and security. This leads us to an inevitable question: <strong>As AI approaches human-level capabilities, are we truly prepared to coexist with it?</strong></p><p>To enhance the reader&#39;s experience, the editors of this chapter have opted <strong>not</strong> to use the traditional news listing format. Instead, they have synthesized insights from multiple domestic and international reports and in-depth analyses, blending <strong>a global perspective with local insights.</strong> This chapter systematically unpacks the contradictions and hopes of our era—<strong>from technological breakthroughs and commercial battles to policy struggles and ethical reflections.</strong> In the following sections, you will discover:</p><ul><li><strong>How Open Source is Breaking Technological Monopolies</strong> Bringing Llama 3 and GPT-4o into direct competition on performance rankings;</li><li><strong>Lightweight Models and Multimodality</strong> Why they have become the key to AI deployment and real-world applications;</li><li><strong>Embodied Intelligence and AI Agents</strong> – How they are redefining the boundaries of human-machine collaboration.</li><li><strong>The Power of Chinese Open-Source AI</strong> How it is leveraging price wars and full-scale open sourcing to break through the competition.</li></ul><p>As we stand at the doorstep of 2025, AI technology is reaching a critical open-source inflection point. The future industry leaders will be those who can strike the optimal balance between open source and commercialization. By embracing open source, AI has the potential to accelerate real-world applications while driving the entire industry toward a more open and inclusive future.Now, more than ever, we must remain clear-eyed: the vast horizon of open-source AI should not be a battleground for a few tech giants but a collective intelligence ecosystem built and shared by all.</p><p>May this report serve as a guiding light, illuminating both reason and warmth in the tide of technological progress.</p><h2 id="_2-2024-global-open-source-ai-current-status-and-development" tabindex="-1">2. 2024 Global Open Source AI: Current Status and Development <a class="header-anchor" href="#_2-2024-global-open-source-ai-current-status-and-development" aria-label="Permalink to &quot;2. 2024 Global Open Source AI: Current Status and Development&quot;">​</a></h2><h3 id="_2-1-2024-open-source-ai-panorama" tabindex="-1">2.1 2024 Open-Source AI Panorama <a class="header-anchor" href="#_2-1-2024-open-source-ai-panorama" aria-label="Permalink to &quot;2.1 2024 Open-Source AI Panorama&quot;">​</a></h3><p>There are different classifications for AI technology stacks and no industry standards, so this section uses some reported classifications for readers&#39; reference.</p><ul><li>Foundation Model</li><li>Model Deployment and Inference</li><li>Development Tools</li><li>Model Training and Fine-tuning</li><li>Monitoring and Observability</li></ul><p>The chart below takes only a selection of products globally as of the second quarter of 2024:</p><p><img src="/2024-China-Open-Source-Report/image/ossAI/HJHilcAD1l.png" alt="image"></p><blockquote><!----></blockquote><h3 id="_2-2-2024-artificial-intelligence-timeline" tabindex="-1">2.2 2024 Artificial Intelligence Timeline <a class="header-anchor" href="#_2-2-2024-artificial-intelligence-timeline" aria-label="Permalink to &quot;2.2 2024 Artificial Intelligence Timeline&quot;">​</a></h3><ul><li><p><strong>Open Source Models - 55</strong><img src="/2024-China-Open-Source-Report/image/ossAI/S16M_0J_yl.png" alt="image"></p></li><li><p><strong>API-Only Models - 63</strong><img src="https://hackmd.io/_uploads/HJ1Ed0ku1x.png" alt="image"></p><blockquote><!----></blockquote></li></ul><h2 id="_3-2024-artificial-intelligence-panorama-report" tabindex="-1">3. 2024 Artificial Intelligence Panorama Report <a class="header-anchor" href="#_3-2024-artificial-intelligence-panorama-report" aria-label="Permalink to &quot;3. 2024 Artificial Intelligence Panorama Report&quot;">​</a></h2><h3 id="_3-1-research-technological-breakthroughs-and-capabilities-of-artificial-intelligence" tabindex="-1">3.1 Research: Technological Breakthroughs and Capabilities of Artificial Intelligence <a class="header-anchor" href="#_3-1-research-technological-breakthroughs-and-capabilities-of-artificial-intelligence" aria-label="Permalink to &quot;3.1 Research: Technological Breakthroughs and Capabilities of Artificial Intelligence&quot;">​</a></h3><ul><li><p><strong>Performance Convergence</strong> OpenAI&#39;s early lead is narrowing, with models like Claude (Anthropic&#39;s AI model) and Gemini (Google&#39;s AI model) catching up. For most of the year, benchmarks and community leaderboards indicated a significant gap between GPT-4 and other top models. However, Claude 3.5 Sonnet, Gemini 1.5, and Grok 2 (Tesla&#39;s AI model) have nearly closed this gap, and model performance is now beginning to converge.</p><p><img src="/2024-China-Open-Source-Report/image/ossAI/ryScvbZukl.png" alt="Foundational Model Performance (Lmsys Leaderboard)"></p><blockquote><!----></blockquote></li><li><p><strong>The Rise of Open Source Models</strong> Meta&#39;s Llama series has significantly narrowed the performance gap with proprietary models. &quot;This marks the first time that open-source models have closed the gap with the cutting-edge closed-source models.&quot; For instance, models like Mistral, Vicuna, and Yi have surpassed closed models in certain aspects. For example, Mixtral 8x7B has outperformed GPT-3.5 in both Elo and MMLU evaluations.</p><p><img src="/2024-China-Open-Source-Report/image/ossAI/SJiIqbW_1e.png" alt="Foundational Model Performance (Open-LLM-Leaderboard)"></p><blockquote><!----></blockquote></li><li><p><strong>Open-source Hosting Platform have accelerated the Momentum</strong></p><p>Platforms such as GitHub and Hugging Face have seen the emergence of many groundbreaking research and development projects with remarkable results.</p><ul><li><p>In 2023, GitHub saw a 148% increase in contributors and a 248% growth in generative AI projects, while Hugging Face hosted over 400,000 models.</p></li><li><p>Popular open source projects in the first half of 2024：</p><ul><li>Huggingface, MindsDB, and Roboflow are the most popular projects on GitHub.</li><li>AutoGPT and ModularML&#39;s Mojo lead GitHub buzz in 2023.</li><li>LeRobot: Offering models, datasets, and tools for real-world robotics applications, dedicated to making robotics more accessible and user-friendly.</li><li>MindsDB, an NVIDIA-backed platform focused on enabling businesses to build AI models using data and simplifying the integration of data sources with AI/ML tools, along with other emerging projects, rapidly gained traction in 2024.</li></ul><p><img src="/2024-China-Open-Source-Report/image/ossAI/HJ_TzzWO1g.jpg" alt="Open Source AI Github Tracction"></p><p><img src="/2024-China-Open-Source-Report/image/ossAI/HkwoXGbd1g.png" alt="Open Source AI Foundation Models Hugging Face Traction"></p></li></ul></li><li><p><strong>The Challenges of Model Benchmarking</strong></p><p>The benchmarking of large language models (LLMs) is akin to the &quot;standardized tests&quot; of the artificial intelligence world, employing uniform questions and criteria to help us fairly compare the performance of different models. Early testing content was relatively simple, such as sentence completion tasks. However, as large models have evolved from &quot;text continuation players&quot; to &quot;programming experts&quot; and &quot;dialogue specialists,&quot; the testing content has become increasingly challenging. Modern benchmarks not only assess the logical capabilities of models but also evaluate their safety, domain-specific knowledge, and other multifaceted performances. It&#39;s like an upgraded examination, more comprehensively assessing the &quot;strength&quot; of the models.</p><ul><li><strong>Better Model Selection</strong>: Benchmarks allow you to choose models based on real-world performance metrics. Want an excellent coding assistant? Check how it performs on HumanEval. Need safety or factual accuracy? Check TruthfulQA or SafetyBench.</li><li><strong>Progress Tracking</strong>: Researchers use benchmarks to understand whether changes (such as fine-tuning or prompt updates) truly improve performance or just add hype.</li><li><strong>Standardization</strong>: With a fair competitive environment, anyone can verify the vendor&#39;s claims. If a new LLM claims to have world-class mathematical reasoning abilities, you can check whether it passed the GSM8K or MATH exams.</li></ul><p>Benchmarking is essentially a set of specially designed &quot;exam questions,&quot; which could be multiple-choice, programming problems, or conversational tasks. Each question has a clear correct answer or scoring standard. We &quot;bring the large model into the exam room&quot; and have it answer these questions. Then, we score it based on how many it answers correctly (or other performance metrics) and compare it with other models on the leaderboard. A high score represents the model&#39;s strengths, while a low score tells us which areas need improvement. It&#39;s like giving AI a comprehensive report card!</p><ul><li>Some benchmarks can become &quot;easier&quot; over time as models improve, thus losing their predictive value.</li><li>Benchmarks rarely simulate the chaotic conditions of the real world. For business use cases, custom tests are better than relying solely on public leaderboards.</li><li>Data leakage is a concern—if a model is trained on the test set, the results may be inflated. Dataset contamination and errors in benchmark testing are affecting progress evaluations and raising security concerns. &quot;Researchers are increasingly focusing on the issue of dataset contamination.&quot; &quot;Some of the most popular benchmarks have surprisingly high error rates, which could lead to underestimating the capabilities of certain models and pose security risks.&quot;</li></ul><p><strong>Evaluation Metrics</strong><img src="/2024-China-Open-Source-Report/image/ossAI/HJGAKR1OJl.png" alt="image"></p><blockquote><!----></blockquote></li><li><p><strong>Focus on Reasoning Computation</strong> OpenAI&#39;s o1 model demonstrates the potential of shifting computation to the inference layer to tackle complex problems, but it also comes with a significant increase in usage costs.</p></li><li><p><strong>Multi-modal and New Architectures</strong> Multimodal models are gaining momentum, while researchers are exploring alternatives to Transformers (though Transformers remain the dominant approach) and hybrid architectures to enhance efficiency and address specific tasks.</p></li><li><p><strong>Synthetic Data</strong> The use of synthetic data is expanding, but concerns about model collapse raised by critics still persist.</p></li><li><p><strong>&quot;Lightweight&quot; Leads the New Trend in AI</strong> In recent years, as the scale of AI models has continued to expand, computing costs have also risen accordingly. To address these challenges, the focus of research has shifted toward improving efficiency and enabling edge-side deployment. For instance, many new technologies are dedicated to compressing model size and reducing computational demands while maintaining performance stability as much as possible. Such optimization is particularly crucial for promoting AI applications in mobile devices and resource-constrained scenarios:</p><ol><li><strong>Model Compression</strong>: Techniques to reduce model size;</li><li><strong>Knowledge Distillation</strong>: Methods to transfer &quot;knowledge&quot; from large models to smaller ones;</li><li><strong>Lightweighting</strong>: Reducing the numerical accuracy of models to reduce storage and computational requirements.</li></ol></li><li><p><strong>Advances in specific scientific fields</strong> AI has brought astonishing advancements across multiple scientific fields, such as predicting protein structures, discovering new drugs, gene editing, materials research, and innovations in robotics and medicine. It can even generate synthetic data in medical imaging. These breakthroughs demonstrate the vast potential of AI, injecting vitality into interdisciplinary research and innovation. The recent awarding of the Nobel Prizes in Physics and Chemistry stands as a testament to AI&#39;s role in driving scientific progress!</p><p><img src="/2024-China-Open-Source-Report/image/ossAI/H1ubqiRwke.png" alt="image"></p><blockquote><!----></blockquote></li><li><p><strong>Planning and Reasoning Limitations</strong> Large language models still &quot;stumble&quot; when handling planning and simulation tasks, reflecting certain limitations in their generalization and real-world reasoning capabilities. However, the latest research is enhancing their reasoning abilities through clever methods, such as &quot;chain-of-thought prompting&quot; (guiding the model to derive answers step by step), reinforcement learning (helping the model learn through trial and error), and open-ended exploration (expanding its adaptability). These approaches are all striving to make models smarter and more reliable!</p></li><li><p><strong>Chinese AI model companies are making steady progress, forging ahead with determination</strong></p><p>China (V)LLM tops the leaderboards despite sanctions.</p><p><strong><!----></strong></p><p><img src="https://hackmd.io/_uploads/SkFDQKr3kl.png" alt="OpenAI o1 racing"></p><blockquote><!----></blockquote><ul><li><p>Alibaba recently released the <strong>Qwen-2</strong> series, whose visual capabilities have particularly impressed the community, ranging from challenging OCR tasks to the ability to analyze complex artworks.</p></li><li><p>In terms of smaller projects:</p><ul><li>The NLP Lab at Tsinghua University has funded the <strong>OpenBMB</strong> project, which gave rise to the <strong>MiniCPM</strong> series. These models, with fewer than 2.5B parameters, can run on devices. Their 28B vision model slightly trails GPT-4V on some metrics, while the Llama 3-based 85B model surpasses GPT-4V on others.</li><li>Tsinghua University&#39;s Knowledge Engineering Group has also developed <strong>CogVideoX</strong>, one of the most capable text-to-video models.</li></ul></li><li><p>On January 20, 2025, two highly anticipated domestic AI large model startups, <strong>DeepSeek</strong> and <strong>Kimi</strong>, almost simultaneously released their latest models: the <strong>DeepSeek-R1</strong> reasoning model and the <strong>Kimi k1.5</strong> multimodal thinking model. Both outperformed OpenAI&#39;s official o1 model in performance benchmarks. Meanwhile, compared to their international counterparts, China&#39;s open-source models are more comprehensive, garnering a global following.</p></li><li><p><strong>DeepSeek</strong></p><ul><li><p><strong>DeepSeek-V3</strong>, built on non-top-tier hardware configurations, has developed a cost-effective and high-performing AI large model that rivals GPT-4o through architectural innovation, all at a fraction of the cost of international tech giants. What&#39;s more, it&#39;s open-source!</p></li><li><p><strong>DeepSeek-V3</strong> has surpassed other open-source models like Qwen2.5-72B and Llama-3.1-405B in multiple benchmarks, achieving performance on par with the world&#39;s top closed-source models, GPT-4o and Claude-3.5-Sonnet. Although the current open-source version does not yet support multimodal input and output, it is highly promising and worth anticipating. <img src="/2024-China-Open-Source-Report/image/ossAI/SJcQq8kOkg.png" alt="image"></p></li><li><p>Even more impressive is that the entire training of <strong>DeepSeek-V3</strong> cost only $5.576 million, significantly lower than the hundreds of millions of dollars spent by OpenAI, Meta, and others on pre-training large language models. <img src="https://hackmd.io/_uploads/SkfPqL1dJl.png" alt="image"></p></li><li><p>On January 20, 2025, the <strong>DeepSeek-R1</strong> reasoning model was released, demonstrating outstanding performance across multiple benchmarks, particularly in mathematics and coding, with some capabilities reaching levels comparable to OpenAI&#39;s o1 model. Its unique feature lies in the adoption of Reinforcement Learning as the core training method, breaking away from traditional reliance on large-scale annotated data. Through this approach, <strong>DeepSeek-R1</strong> exhibits strong reasoning abilities, including associative thinking and self-verification.</p></li><li><p>With its open-source nature and a cost 20 times lower, it stands alongside ChatGPT-4o (as of November 20, 2024). <img src="/2024-China-Open-Source-Report/image/ossAI/SkOUTMMOke.png" alt="image"></p></li><li><p>On the leaderboard for complex prompts/style control, <strong>DeepSeek-R1</strong> ranks first. <img src="/2024-China-Open-Source-Report/image/ossAI/H1r5ofMuyl.jpg" alt="DeepSeek R1 Chatbot Arena - Hard Prompts w. Style Control"></p></li><li><p>Based on current results, <strong>DeepSeek-R1</strong> maintains a leading position across all dimensions. <img src="/2024-China-Open-Source-Report/image/ossAI/rJovoMGd1g.jpg" alt="DeepSeek R1 Overall Comparison"></p></li><li><p>On the WebDev sub-ranking, which measures a model&#39;s programming and development capabilities, <strong>DeepSeek-R1</strong> ranks second, trailing the closed-source Claude 3.5 Sonnet by less than 40 points.</p></li></ul><p><img src="/2024-China-Open-Source-Report/image/ossAI/S1Cx2zzuJg.jpg" alt="DeepSeek R1 WebDev Leaderboard"></p></li><li><p>Just before the Year of the Snake, Moonshot AI unveiled the <strong>Kimi k1.5</strong> multimodal thinking model, creating a sensation in the AI community. <strong>Kimi k1.5</strong> excels in short-chain reasoning and multimodal processing, while OpenAI&#39;s o1 maintains a strong performance in long-chain reasoning and broad application scenarios. Each has its strengths, catering to different application needs. <br></p></li></ul></li></ul><table><thead><tr><th>Feature</th><th>Kimi k1.5</th><th>OpenAI o1</th></tr></thead><tbody><tr><td><strong>Multimodal Capability</strong></td><td>Supports joint text and visual reasoning</td><td>Primarily focused on text processing</td></tr><tr><td><strong>Short-chain Reasoning Performance</strong></td><td>Surpasses GPT-4o and Claude 3.5 Sonnet</td><td>Excellent, but not as good as Kimi k1.5</td></tr><tr><td><strong>Long-chain Reasoning Performance</strong></td><td>Matches the full version of o1</td><td>Significant advantage in long-chain reasoning</td></tr><tr><td><strong>Context Window</strong></td><td>128K</td><td>&lt;=128K</td></tr><tr><td><strong>Reinforcement Learning Application</strong></td><td>Widely applied, enhancing autonomous exploration capabilities</td><td>Present, but not exceeding Kimi k1.5</td></tr></tbody></table><!----><h3 id="_3-2-industry-commercial-applications-of-artificial-intelligence-and-its-business-impact" tabindex="-1">3.2 Industry: Commercial Applications of Artificial Intelligence and Its Business Impact <a class="header-anchor" href="#_3-2-industry-commercial-applications-of-artificial-intelligence-and-its-business-impact" aria-label="Permalink to &quot;3.2 Industry: Commercial Applications of Artificial Intelligence and Its Business Impact&quot;">​</a></h3><ul><li><p><strong>Business User Preferences</strong> 41% of business users prefer open source AI because it is more cost-effective, has more room for innovation, and has fewer legal constraints.</p></li><li><p><strong>NVIDIA&#39;s Dominance</strong> NVIDIA remains the undisputed leader in the AI chip market—its GPUs are almost ubiquitous in research papers and large-scale deployments. With rapid product updates and advanced GPU interconnect technology, NVIDIA&#39;s position has become even more solid. Even NVIDIA&#39;s CEO, Jensen Huang, boldly suggested, &quot;Every government should have its own large language model to protect national assets.&quot;</p><p>However, this heavy reliance on NVIDIA has sparked some concerns. As a result, tech giants like Google, Meta, and OpenAI have begun actively exploring alternative hardware solutions, seeking more options to break free from the constraints of a single supplier.</p><ul><li>Competitors: Established rivals like AMD and Intel are striving to catch up but remain significantly behind NVIDIA.</li><li>A handful of AI chip startups, such as Cerebras and Groq, are gaining some momentum. Unlike NVIDIA, which dominates the training hardware market, these startups primarily focus on providing inference interfaces and cloud services, differentiating themselves by offering faster and more cost-effective solutions than NVIDIA.</li></ul></li><li><p><strong>Talent Flow from Major Labs</strong> Top AI labs like OpenAI and DeepMind have recently experienced a brain drain, enabling well-funded newcomers such as Sakana AI and H Company to rise rapidly. These new challengers, led by renowned researchers, focus on specific AI domains or innovative architectures, showcasing the diversification and deepening of the AI ecosystem.</p><p>However, transitioning from research to entrepreneurship is not always smooth sailing. Some high-profile projects have already encountered significant challenges in practical operations, highlighting the gap between technological breakthroughs and commercial implementation.</p></li><li><p><strong>Penetration into Vertical Industries</strong> AI is making significant strides across various sectors! Even the traditionally &quot;conservative&quot; legal industry is finally seeing the large-scale application of legal tech. In programming, AI development tools like GitHub Copilot have become invaluable assistants for many programmers. In autonomous driving, Wayve and Waymo are steadily advancing, while Cruise has recently encountered some challenges.</p><p>In the medical field, AI is also gaining momentum: mRNA-based personalized cancer therapies are showing immense potential, while AI-driven drug development companies are continuously integrating resources and growing rapidly, injecting more innovation into the healthcare industry!</p></li><li><p><strong>Emerging Technologies &amp; Applications</strong> Some emerging AI fields are rapidly rising, brimming with opportunities and challenges. For instance, humanoid robot companies have attracted substantial investments, but to truly materialize, they must overcome both technological and market hurdles.</p><p>Simultaneously, breakthroughs in voice-to-voice AI technology have enabled conversations that are nearly on par with human levels, sparking a wave of anticipation for the future of voice interaction. In the realm of video generation, multiple contenders are vying to enhance the quality of technology and its scalable application.</p><p>However, some AI-driven consumer products, such as the Rabbit R1 and Humane AI Pin, have not lived up to expectations. This underscores the fact that there are still numerous challenges to overcome in transitioning from impressive technology to practical, user-friendly products.</p></li><li><p><strong>Ethical and Legal Challenges</strong> With the widespread application of AI technology, issues surrounding copyright and legality are increasingly coming to the forefront. Model developers are often questioned by content creators and media companies regarding the legality of their data usage. However, courts have yet to provide a clear stance on what constitutes &quot;fair use&quot; in AI training.</p><p>Additionally, a new phenomenon known as &quot;pseudo-acquisition&quot; is beginning to spark debate. This practice involves large tech companies hiring only the core team of an AI startup rather than acquiring the entire company. Such maneuvers have already caught the attention of regulatory bodies, and it is likely that more rules addressing this issue will be introduced in the future.</p></li><li><p><strong>AI Economics</strong> An increasing number of enterprises are adopting &quot;AI-first&quot; products, which boast strong user retention, and the revenue growth of AI companies far outpaces that of traditional SaaS companies. However, many AI firms are still exploring how to establish a truly sustainable profit model.</p><p>Meanwhile, several emerging innovative fields are rapidly gaining momentum, brimming with vitality and opportunities:</p><ul><li><p><strong>The Post-Sora Era of AI Video</strong> The field of AI video has entered a phase of rapid development following Sora, branching into two main trajectories: closed-source models and open-source models:</p><ul><li><strong>Closed-Source Models</strong>: These include a series of models (such as Gen-3, D.M, Kling, etc.) launched by technology companies like Runway, LUMA AI, ByteDance, and Kuaishou, which have gradually evolved to higher versions (e.g., D.M 1.6 and Kling 1.5). These models continue to enhance their functionality and precision, offering increasingly robust support for video generation.</li><li><strong>Open-Source Models</strong>: Centered around the Open-Sora series and complemented by emerging projects like EasyAnimate and the CogVideo-X series, these models are driving the growth of the open-source community. They provide developers with more accessible tools and opportunities for collaboration.</li></ul><p>The following diagram illustrates the key milestones and version iteration trajectory of AI video technology from mid-2024 to the end of the year. It highlights the parallel advancement of open-source and closed-source models, as well as the gradual maturation of technology implementation and application.</p><p><img src="/2024-China-Open-Source-Report/image/ossAI/SyLtBGWdkg.png" alt="image"></p><blockquote><p><a href="https://ailab-cvc.github.io/VideoGen-Eval/" target="_blank" rel="noreferrer">https://ailab-cvc.github.io/VideoGen-Eval/</a></p></blockquote></li><li><p><strong>AI Coding</strong> As shown in the figure below, AI programming has gradually become mainstream. Notable AI programming tools include Cursor and Devin internationally, while domestically, there are ByteDance&#39;s Doubao MarsCode, Alibaba&#39;s Tongyi Lingma, and Baidu&#39;s Wenxin Kuaima, among others.</p><p><img src="/2024-China-Open-Source-Report/image/ossAI/r1G1QQbOJg.png" alt="image"></p></li><li><p><strong>AI Search</strong> Startups like Perplexity are leading the charge by leveraging AI to provide precise summaries for search queries, sparing users from being overwhelmed by hundreds of links. This approach has rapidly attracted a global user base, propelling the company to a valuation of $9 billion. Meanwhile, tech giants such as Google, Bing, and ChatGPT 4o are also making significant strides in the AI search domain, setting the stage for an intensely competitive future.</p></li><li><p><strong>AI Consumer Hardware</strong> However, some AI-driven consumer devices, such as the smart glasses developed through the collaboration between Meta and Rayban, have initially garnered market attention and acceptance. Domestically, the launch of the <strong>ShineGo AI glasses</strong> at the end of 2024 marks a significant advancement in the smart wearable device market in China. Their multifunctionality and convenience make them competitive in the marketplace.</p><p>Of course, some devices, such as the Rabbit R1 and Humane AI Pin, have failed to meet expectations, highlighting the challenges of translating AI technology into practical consumer products.</p></li><li><p><strong>Embodied AI and AI Agents</strong> will be discussed in detail in later chapters.</p></li></ul></li><li><p><strong>Investment Boom</strong></p><p><strong>The AI investment boom continued in 2024</strong>, with total investment nearing $100 billion. Notably, a clear watershed has emerged between the &quot;pre-GPT-4&quot; and &quot;post-GPT-4&quot; eras, with large-scale funding rounds of $250 million or more dominating the landscape. The total valuation of AI companies has soared to nearly $9 trillion, primarily driven by a handful of publicly traded firms.</p><p>However, the IPO market remains stagnant, and merger and acquisition activities are declining, reflecting regulatory pressures and market uncertainties.</p><p><strong>Capital flow in the open-source AI sector has significantly accelerated</strong>, with large-scale funding deals becoming the norm. Over the past two years, this sector has completed more than 60 transactions, totaling over $13 billion. Remarkably, 45% of these deals were Series A or later-stage funding rounds, indicating that investors are increasingly focusing on projects that have entered the growth phase, optimistic about their potential and future prospects.</p><p><strong>Open-Source AI Startups Attract Significant Capital Investment</strong></p><ul><li>Scale AI completed a $1 billion Series F funding round.</li><li>Mistral AI secured $640 million in a Series B funding round.</li><li>Together AI raised $106 million in a Series A funding round.</li><li>Supabase and AutoGPT are also seen as having the potential to attract further investment.</li></ul><p><strong>YC Investment in Startups (Approximately 250+ Companies)</strong> Y Combinator (commonly referred to as YC) is a highly renowned startup accelerator in the United States. Since its founding in 2005, it has been hailed as the &quot;cradle of top-tier startups.&quot; YC hosts two startup programs each year: the Winter Batch (January to March) and the Summer Batch (June to August), providing initial investment and resource support to participating startups.</p><p>YC&#39;s startup accelerator culminates in the final Demo Day. On Demo Day, all teams present their results of going from zero to one to investors, while also quickly establishing connections with numerous investment firms. It can be said that this is a crucial moment for startups to gain investment and exposure.</p><p>All AI projects in the Summer 2024 (S24) accelerator fully demonstrate that the AI startup boom in Silicon Valley is continuing to heat up!</p><p><img src="/2024-China-Open-Source-Report/image/ossAI/HJ5HYGWukx.png" alt="image"></p><blockquote><p><a href="https://www.ycombinator.com/companies?batch=S24" target="_blank" rel="noreferrer">https://www.ycombinator.com/companies?batch=S24</a></p></blockquote></li><li><p><strong>Acquisitions Surge</strong></p><ul><li>Nvidia acquired Deci AI for $300 million</li><li>Other acquisitions:</li></ul><!----> **2024 AI Company Acquisition Statistics** </li></ul><table><thead><tr><th><strong>Report Date</strong></th><th><strong>Company</strong></th><th><strong>Valuation</strong></th><th><strong>Description</strong></th><th><strong>Acquisition News</strong></th></tr></thead><tbody><tr><td><strong>July 2024</strong></td><td><strong>Adept</strong></td><td>Over $1B</td><td>AI Agent</td><td>Acquired by Amazon, with talent and technology integrated into Amazon</td></tr><tr><td><strong>July 2024</strong></td><td><strong>Leonardo.Ai</strong></td><td>Not disclosed</td><td>AI platform for text-to-image generation</td><td>Acquired by Canva; will continue as an independent brand while integrating with Canva</td></tr><tr><td><strong>May 2024</strong></td><td><strong>Character.ai</strong></td><td>$1B</td><td>AI-powered chatbot</td><td>Recently sought partnerships with Meta and xAI</td></tr><tr><td><strong>May 2024</strong></td><td><strong>Humane</strong></td><td>$750M - $1B</td><td>AI consumer hardware provider</td><td>Considering a sale to Apple for approximately $1B</td></tr><tr><td><strong>May 2024</strong></td><td><strong>Reka AI</strong></td><td>$300M</td><td>Generalist AI model provider</td><td>Snowflake attempted to acquire Reka AI for over $1B, but the deal reportedly fell apart</td></tr><tr><td><strong>May 2024</strong></td><td><strong>Wonder Dynamics</strong></td><td>Not disclosed</td><td>AI tool for mainstream CG production</td><td>Acquired by design software company Autodesk</td></tr><tr><td><strong>May 2024</strong></td><td><strong>Stability AI</strong></td><td>$4B</td><td>Open-source AI platform for text-to-image generation</td><td>Has faced internal crises due to talent loss since last year</td></tr><tr><td><strong>March 2024</strong></td><td><strong>Inflection AI</strong></td><td>Over $4B</td><td>AI model and AI assistant development</td><td>Acquired by Microsoft, with the founder joining Microsoft</td></tr><tr><td><strong>February 2024</strong></td><td><strong>Writer</strong></td><td>$500M - $750M</td><td>Enterprise-grade AI writing assistant</td><td>Being pursued by three enterprises for acquisition</td></tr><tr><td><strong>February 2024</strong></td><td><strong>Perplexity</strong></td><td>$3B</td><td>AI search tool</td><td>Its management is in acquisition discussions with four companies</td></tr></tbody></table><ul><li><p><strong>AI Graveyard</strong></p><p>Starting a business is tough, and AI is especially challenging: The famous AI Graveyard quickly records a large number of fallen AI startups. However, we can expect that in the future, many more entrepreneurs and investment firms will continue to rush into the AI innovation field. AI entrepreneurs never die; they just fade away!</p><p><img src="/2024-China-Open-Source-Report/image/ossAI/HkxgAQW_1x.png" alt="image"></p><blockquote><p>Source：<a href="https://dang.ai/ai-graveyard" target="_blank" rel="noreferrer">https://dang.ai/ai-graveyard</a></p></blockquote></li></ul><h3 id="_3-3-policy-ai-regulation-economic-impact-and-the-evolving-geopolitics-of-ai" tabindex="-1">3.3 Policy: AI Regulation, Economic Impact, and the Evolving Geopolitics of AI <a class="header-anchor" href="#_3-3-policy-ai-regulation-economic-impact-and-the-evolving-geopolitics-of-ai" aria-label="Permalink to &quot;3.3 Policy: AI Regulation, Economic Impact, and the Evolving Geopolitics of AI&quot;">​</a></h3><ul><li><p><strong>US AI Regulatory Trends</strong> The Biden administration issued an executive order to regulate the most advanced AI models. Specifically, it requires models trained with more than 1026 FLOPS of computing power to report to the federal government and submit security test results before going public. However, since this order is an executive order, it can be easily revoked by future administrations, so its long-term impact remains uncertain.</p><p>Meanwhile, individual states have started to introduce their own AI regulations, with California&#39;s SB 1047 bill being the most comprehensive but also the most controversial. However, due to strong opposition from many Silicon Valley leaders, the bill was ultimately vetoed by the Governor of California.</p></li><li><p><strong>EU AI Act Passed</strong> After long discussions and lobbying, the European Parliament finally passed the AI Act, becoming the first comprehensive AI regulatory framework in the world. The act will be implemented in phases and adopts a tiered management approach for foundational models.</p><p>However, major US tech companies have faced some challenges in adapting to these new regulations in the EU. For example, Anthropic&#39;s AI model, Claude, was not available to European users until May 2024, and Meta has temporarily withheld multimodal models from the European market. These examples highlight that multinational tech companies still need further adjustments and adaptations to comply with the EU&#39;s strict AI regulations.</p></li><li><p><strong>Data Collection Scrutiny</strong> As the demand for data grows among model developers, the policies surrounding the collection of user data have come under increasing scrutiny and attention. For example, Meta was required to offer EU users an option to &quot;opt out of global data participation,&quot; while X (formerly Twitter) has ceased using public posts from European users to train AI models. These changes reflect the growing emphasis on protecting user privacy.</p></li><li><p><strong>Global AI Competitive Landscape</strong> The Japanese government is actively supporting AI entrepreneurship and venture capital, hoping to inject new vitality into the economy through the AI boom. At the same time, as cutting-edge AI laboratories face increasing funding demands, some sovereign wealth funds are starting to play a more significant role in AI investments. This situation has also raised concerns about national security, as some of the funding may come from abroad, increasing the potential risks related to sensitive technologies.</p></li><li><p><strong>Public Computing Power Infrastructure</strong> The UK, US, and EU are all working to increase the supply of public computing resources, but compared to private companies, the resources available are still somewhat limited. Meanwhile, the Indian government has proposed an ambitious plan: they are willing to fund half of the costs to help build large-scale GPU clusters, with the condition that private companies cover the remaining expenses. This collaborative model opens up new possibilities for cooperation between the public and private sectors.</p></li><li><p><strong>Energy Consumption Issues</strong> With the rapid development of AI, large tech companies are struggling to meet the challenge of achieving &quot;net-zero emissions&quot; due to a significant increase in energy consumption. At the same time, the growth of AI has put a strain on existing energy infrastructure. Some countries have even started to restrict the construction of new data centers in order to control the rapid growth in energy demand.</p></li></ul><h3 id="_3-4-safety-identifying-and-mitigating-the-catastrophic-risks-of-future-high-capability-ai-systems" tabindex="-1">3.4 Safety: Identifying and Mitigating the Catastrophic Risks of Future High-Capability AI Systems <a class="header-anchor" href="#_3-4-safety-identifying-and-mitigating-the-catastrophic-risks-of-future-high-capability-ai-systems" aria-label="Permalink to &quot;3.4 Safety: Identifying and Mitigating the Catastrophic Risks of Future High-Capability AI Systems&quot;">​</a></h3><ul><li><p><strong>Shift in AI Safety Attitudes</strong> In 2024, discussions surrounding AI safety saw an interesting shift. The concerns of &quot;AI is dangerous&quot; from 2023 evolved into a more positive attitude of &quot;come use my AI application&quot; in 2024. OpenAI&#39;s &quot;internal turmoil&quot; further signified the beginning of a backlash against the narrative of &quot;AI poses risks.&quot;</p><p>Despite the more optimistic atmosphere, governments around the world have not slowed down their efforts to govern AI safety. The UK was the first to establish the world’s first AI Security Institute (AISI, was AI Safety Institute), followed by the US, Japan, and Canada, all of whom are working to ensure that AI development is both efficient and safe.</p></li><li><p><strong>New Focus in AI Safety Research</strong> Researchers are increasingly focusing on &quot;jailbreaking&quot; attacks on AI systems. Although major labs have ramped up research into defending against these types of attacks, red team tests continue to successfully breach AI defenses.</p><p>What’s even more interesting is that research has revealed more subtle forms of attack, such as &quot;sneaky tampering&quot; with the preference data used to train AI models (RLHF, or reinforcement learning with human feedback), subtly manipulating the behavior of the model. These findings indicate that AI safety remains an ongoing cat-and-mouse game.</p></li><li><p><strong>Progress and Limitations of Alignment Techniques</strong> In the alignment techniques aimed at making AI more &quot;obedient,&quot; there have been several new attempts and challenges. For example, Direct Preference Optimization (DPO) has gained attention as an alternative to RLHF (Reinforcement Learning with Human Feedback), but research has found that it may encounter similar &quot;over-optimization&quot; issues.</p><p>At the same time, due to the advantage of supporting online learning, RLHF is still unlikely to be fully replaced in the short term. To better address these issues, researchers are trying to combine the strengths of both approaches. For example, they are exploring a new method called &quot;Direct Alignment Architecture Framework&quot; (DAAF), which aims to find more efficient and stable solutions.</p></li><li><p><strong>Breakthroughs in AI Explainability</strong> Anthropic recently successfully used Sparse Autoencoders (SAE) to analyze the internal activations of Claude 3 Sonnet, giving us a clearer understanding of how the model &quot;thinks.&quot; Following that, OpenAI improved SAE technology, making it applicable to larger-scale models.</p><p>These advancements make it possible to &quot;open the AI black box,&quot; helping to reveal the internal workings of the model. However, this technology also raises some concerns—if misused, it could lead to privacy or security risks, reminding us that while pursuing transparency, we must also exercise caution.</p></li><li><p><strong>Biological Risks</strong><br> Previous research by Anthropic raised concerns that large language models (LLMs) could accelerate the emergence of biological threats, but other labs have struggled to replicate this result. However, researchers also emphasize that, compared to LLMs, tools specifically designed for biological purposes (such as protein folding, design, and gene-editing tools) may pose more direct risks. Therefore, focusing on the safe use of these tools is becoming even more important.</p></li><li><p><strong>The Threat of AI Misuse</strong> Research from Google DeepMind points out that most instances of AI misuse are not due to complex hacking techniques, but rather because certain capabilities of generative AI are too easily accessible. For example, deepfake technology has been used for fraud, harassment, and even the creation of illegal pornography. These issues have become a growing area of concern.</p></li></ul><h3 id="_3-5-looking-forward" tabindex="-1">3.5 Looking Forward <a class="header-anchor" href="#_3-5-looking-forward" aria-label="Permalink to &quot;3.5 Looking Forward&quot;">​</a></h3><ul><li><p><strong>Closing the Gap Between Open-Source and Closed-Source Models</strong> Mistral and Llama, among other open-source models, have firmly established themselves within the AI community. By delivering efficient versions and demonstrating exceptional performance in benchmarks, they are competing with top-tier closed-source models like GPT and Claude, even surpassing them in certain metrics.</p></li><li><p><strong>The Role of Open Source in the AI Lifecycle</strong></p><ul><li>Open source continues to grow along the &quot;enlightenment slope&quot; of the AI cycle, driving value innovation.</li><li>While Hugging Face and OpenAI dominate the field, codebases like MindsDB and Roboflow are also rising, with projects such as AutoGPT standing out prominently.</li></ul></li><li><p><strong>Open-Source Innovation Expands Downstream</strong></p><ul><li>Extending from model and tool development to training and monitoring fields.</li><li>In 2024, around 150 companies are involved in open-source AI innovation, including model training, fine-tuning, and monitoring tools, indicating the maturation of the foundational toolchain.</li></ul></li><li><p><strong>Significant Growth in Funding and Deals</strong></p><ul><li>The transaction size and number of deals for open-source AI startups have surged, demonstrating immense potential.</li><li>For example, Nvidia acquired Deci AI for $300 million, Scale AI raised $1 billion, and Mistral AI secured $640 million, highlighting the high recognition of capital in this field.</li></ul></li></ul><h3 id="_3-6-future-predictions-10-predictions-for-ai-developments-in-the-next-12-months" tabindex="-1">3.6 Future Predictions: 10 predictions for AI developments in the next 12 months <a class="header-anchor" href="#_3-6-future-predictions-10-predictions-for-ai-developments-in-the-next-12-months" aria-label="Permalink to &quot;3.6 Future Predictions: 10 predictions for AI developments in the next 12 months&quot;">​</a></h3><ol><li><strong>Massive Investment Draws Attention</strong>: A sovereign nation might invest over $10 billion in a major U.S. AI lab, triggering national security reviews.</li><li><strong>No-Code Creativity Goes Viral</strong>: An app or website entirely created by someone without programming skills could go viral, potentially breaking into the top 100 on the App Store.</li><li><strong>Shift in Data Collection Policies</strong>: As copyright cases go to trial, leading AI labs may make significant changes to their data collection practices.</li><li><strong>EU Relaxes AI Act</strong>: Due to lawmakers&#39; concerns about stifling innovation, the early implementation of the EU AI Act might be more lenient than expected.</li><li><strong>Open-Source AI Breaks Through</strong>: An open-source model could surpass OpenAI&#39;s o1 model in benchmark tests (Editor&#39;s Note: DeepSeek and Kimi are making strides in this direction).</li><li><strong>NVIDIA&#39;s Dominance Remains Unshaken</strong>: Challengers still cannot disrupt NVIDIA&#39;s leadership in the AI chip market.</li><li><strong>Cooling Investment in Humanoid Robots</strong>: Investment in humanoid robots may decline as companies struggle to find viable market directions.</li><li><strong>Apple Advances On-Device AI</strong>: Apple&#39;s breakthroughs in on-device AI research will accelerate the development of AI for personal devices.</li><li><strong>AI Scientist Publishes Research</strong>: A research paper generated by AI might be accepted by a major machine learning conference or workshop.</li><li><strong>AI Video Game Takes Off</strong>: A video game featuring generative AI-driven interactive elements could become a hit, offering a revolutionary gaming experience.</li></ol><h2 id="_4-2024-ai-status-and-development-in-china" tabindex="-1">4. 2024 AI Status and Development in China <a class="header-anchor" href="#_4-2024-ai-status-and-development-in-china" aria-label="Permalink to &quot;4. 2024 AI Status and Development in China&quot;">​</a></h2><p>Compared to the <strong>“Hundred-Model Battle”</strong> of 2023, which left the industry dazzled, the AI market landscape has finally become clearer after a year of reshuffling.</p><p>Major internet giants have now established relatively mature large-model capabilities. While aggressively expanding their cloud businesses, they have also deeply integrated AI models into their product ecosystems. Not only have they launched new AI applications, but many existing products have also been seamlessly aligned with large models—an important strategy for strengthening their <strong>competitive moat</strong>.</p><p>Meanwhile, China’s <strong>“Six AI Tigers”</strong>—the startups aiming to compete with OpenAI—have spent the past year navigating challenging or successful funding rounds, securing their spots at the AI table. Each company is now carving out its own path in model development and application, showcasing diverse commercialization strategies.</p><p>Additionally, startups in niche areas such as <strong>edge AI models</strong> and <strong>video generation models</strong> have demonstrated remarkable vitality. Though they may not match the scale of the tech giants, they have emerged as rising stars in their respective domains, with the potential to dominate their specialized tracks.</p><p>After a year of consolidation, the industry is transitioning from chaos to stability. Tech giants continue their steady expansion, leveraging their vast resources, while startups are making bold moves in niche markets, injecting fresh energy and diversity into the AI ecosystem.</p><h3 id="_4-1-china-s-ai-startup-leader" tabindex="-1">4.1 China&#39;s AI startup leader <a class="header-anchor" href="#_4-1-china-s-ai-startup-leader" aria-label="Permalink to &quot;4.1 China&#39;s AI startup leader&quot;">​</a></h3><p><strong>Top Domestic Large AI Models</strong> Companies with Over 1 Billion RMB in Funding</p><table><thead><tr><th>Company</th><th>2024 Financing / Valuation</th><th>Total Raised Funds</th><th>Investors</th></tr></thead><tbody><tr><td>Zhipu AI (智谱 AI)</td><td>March: Several billion RMB financing; June: Over $400M financing; September: Several billion RMB financing, with a total investment exceeding 20 billion RMB; December: $301M financing</td><td>Over 50 billion RMB</td><td>Beijing AI Industry Investment Fund, Mubadala Capital (UAE), Prosperity7, Keywin Capital, Junlian Capital, and other institutional investors</td></tr><tr><td>Moonshot AI (月之暗面)</td><td>March: 1 billion RMB financing; August: Over $3B valuation</td><td>Over $2B (~150 billion RMB)</td><td>Alibaba, Sequoia China, Xiaomi, Meituan, Hillhouse Capital, Tencent Investment, and other top-tier investors</td></tr><tr><td>Baichuan Intelligence (百川智能)</td><td>July: $500M in Series A funding; Soon to complete Series B with a $2B valuation</td><td>Over 7.5 billion RMB</td><td>Investors include Alibaba, Xiaomi, Tencent, and other major companies and financial investment institutions, along with the Beijing AI Industry Investment Fund, Sequoia, and other investors</td></tr><tr><td>MiniMax</td><td>March: Over $600M financing; Recent investment will push MiniMax valuation beyond $2.5B</td><td>Over 6 billion RMB</td><td>Alibaba, Sequoia, and top-tier investment institutions</td></tr><tr><td>01.AI (零一万物)</td><td>August: New multi-hundred-million-dollar financing round completed</td><td>Over 10 billion RMB</td><td>Investors include Hillhouse International Strategy and Southeast Asia financial groups</td></tr><tr><td>StepFun (阶跃星辰)</td><td>December: Several hundred million USD Series B financing</td><td>Over 10 billion RMB</td><td>Core investors include Shanghai Lingang Investment and Sequoia, with strategic capital from Tencent Investment, Source Code Capital, and others</td></tr><tr><td>Beijing Wenge Technology (中科闻歌)</td><td>December: Several billion RMB strategic financing</td><td>Over 10 billion RMB</td><td>Beijing AI Industry Investment Fund</td></tr><tr><td>ModelBest (面壁智能)</td><td>December: Several hundred million RMB financing completed</td><td>Over 10 billion RMB</td><td>Led by Cambricon, Denglin Technology, Zhongke Chuangshi Fund, and Shunwei Capital, along with Beijing AI Industry Investment Fund and Yuanjing Investment, with Vanke as an independent investment consultant</td></tr></tbody></table><blockquote><p>Note: Statistics based on open-source information, data as of December 31, 2024.</p></blockquote><p><strong>Major Product Lines of the Six Emerging AI Model Companies</strong></p><p><img src="/2024-China-Open-Source-Report/image/ossAI/HJX1eV-_kx.png" alt="image"></p><table><thead><tr><th>Company</th><th>To C Products</th><th>To B Products &amp; Solutions</th></tr></thead><tbody><tr><td>Zhipu AI (智谱)</td><td>Zhipu Qingyan (智谱清言)</td><td>BigModel Zhipu AI Open Platform: Solutions for smart vehicles, manufacturing, consumer goods, finance, government services, healthcare, gaming, cultural tourism, and AI agents.</td></tr><tr><td>MiniMax</td><td>HaiLuo AI (海螺AI), Xingye (星野), Talkie</td><td>MiniMax Open Platform: Solutions for office, entertainment, infrastructure services, and smart hardware</td></tr><tr><td>Baichuan Intelligence (百川智能)</td><td>BaiXiaoYing (百小应)</td><td>Baichuan Large Model Open Platform: Domain-enhanced models, full-chain domain enhancement toolchains, AI platforms, and tool-based applications. Industry solutions for healthcare, education, finance, manufacturing, and retail</td></tr><tr><td>StepFun (阶跃星辰)</td><td>Yilian (跃问), Maopeng (冒泡鸭)</td><td>StepFun Open Platform: Solutions for e-commerce, content creation, smart vehicles, local services, finance, AI manufacturing, gaming, and AI governance</td></tr><tr><td>Moonshot AI (月之暗面)</td><td>Kimi</td><td>Kimi Open Platform</td></tr><tr><td>01.AI (零一万物)</td><td>Wanzhi (万知)</td><td>01.AI Large Model Open Platform: Solutions for digital humans, marketing short video solutions, and AI infrastructure solutions</td></tr></tbody></table><!----><h3 id="_4-2-chinese-ai-vendors-start-a-global-price-war" tabindex="-1">4.2 Chinese AI vendors start a global price war <a class="header-anchor" href="#_4-2-chinese-ai-vendors-start-a-global-price-war" aria-label="Permalink to &quot;4.2 Chinese AI vendors start a global price war&quot;">​</a></h3><p>The global price war was ignited by DeepSeek&#39;s astonishing performance and ultra-low pricing, prompting ByteDance&#39;s Doubao and Alibaba&#39;s Qwen to quickly follow suit, slashing prices to the bone and even pushing into negative margins!</p><p><img src="/2024-China-Open-Source-Report/image/ossAI/en/Hy70wzWdkl.png" alt="price_war_comparison"></p><blockquote><!----></blockquote><p><img src="/2024-China-Open-Source-Report/image/ossAI/en/HJR3UG-dyx.jpg" alt="inference_price_comparison"></p><blockquote><!----></blockquote><h2 id="_5-world-modeling-and-spatial-intelligence" tabindex="-1">5. World Modeling and Spatial Intelligence <a class="header-anchor" href="#_5-world-modeling-and-spatial-intelligence" aria-label="Permalink to &quot;5. World Modeling and Spatial Intelligence&quot;">​</a></h2><p>500 million years ago, the emergence of vision shattered the darkness of the world, triggering one of the most profound evolutions in the history of life. Over the past decade, the development of artificial intelligence has been equally astonishing. As we begin to endow computers and robots with &quot;spatial intelligence,&quot; much like the dawn of biodiversity in nature, this could potentially spark a &quot;digital Cambrian explosion.&quot; As this revolution unfolds, the future of AI is brimming with limitless possibilities, filling us with anticipation and excitement!</p><h3 id="_5-1-world-modeling" tabindex="-1">5.1 World Modeling <a class="header-anchor" href="#_5-1-world-modeling" aria-label="Permalink to &quot;5.1 World Modeling&quot;">​</a></h3><p>The world model can autonomously understand how the world operates by learning from unlabeled data (similar to listening to someone speak without needing explanations), completely without requiring explicit instructions.</p><p>The architecture of the world model consists of six modules, each responsible for different tasks:</p><ol><li><strong>Configurator</strong>: Manages and controls overall operations.</li><li><strong>Perception Module</strong>: Understands the current state, acting as the AI&#39;s &quot;sensory organs.&quot;</li><li><strong>World Model</strong>: Predicts what might happen next, akin to the AI&#39;s brain speculating about the future.</li><li><strong>Cost Module</strong>: Calculates the potential costs of each choice, helping to evaluate gains and losses.</li><li><strong>Action Module</strong>: Plans the next steps of action, ensuring clear objectives.</li><li><strong>Short-Term Memory Module</strong>: Tracks the current state and related cost information, ensuring the AI doesn&#39;t &quot;short-circuit.&quot;</li></ol><p>This set of modules functions like a small team, working together to help the AI better understand and respond to the dynamic changes in the world! <img src="/2024-China-Open-Source-Report/image/ossAI/rJlXM2APke.png" alt="image"></p><h3 id="_5-2-spatial-intelligence" tabindex="-1">5.2 Spatial Intelligence <a class="header-anchor" href="#_5-2-spatial-intelligence" aria-label="Permalink to &quot;5.2 Spatial Intelligence&quot;">​</a></h3><p>From a technological development perspective, <strong>World Models</strong> bring a completely new approach to the field of artificial intelligence. The key idea is to convert the perceived information into an abstract &quot;world map&quot; that helps AI understand and predict the dynamic changes in its surrounding environment.</p><p>The core design concept of this model is to use historical data to construct a digital framework that simulates the real-world environment. In other words, a world model functions like a smart &quot;virtual laboratory,&quot; allowing AI to learn, predict, and adapt to reality more efficiently within this digital world.</p><h3 id="_5-3-application-examples-partial" tabindex="-1">5.3 Application Examples (partial) <a class="header-anchor" href="#_5-3-application-examples-partial" aria-label="Permalink to &quot;5.3 Application Examples (partial)&quot;">​</a></h3><ul><li><p><strong>Urban Planning</strong> Tokyo is building its own 3D digital twin city with an accuracy that can achieve an absolute position error of less than 10 centimeters! This digital model not only includes LiDAR point clouds (high-precision laser scan data), but also detailed CityGML data (used to represent city buildings and structures), as well as real-time traffic dynamics.</p><p>According to Japan&#39;s plans, by 2030, Tokyo will become a fully digital twin city, where seamless integration of information will cover everything from transportation to energy. By then, more and more buildings, homes, and factories will be transformed into virtual data, driving a comprehensive upgrade in urban management and planning!</p><p><img src="/2024-China-Open-Source-Report/image/ossAI/BJ60emz_kg.jpg" alt="东京智能空间"></p></li><li><p><strong>Traffic Management and Autonomous Driving</strong></p><ul><li><p><strong>New South Wales Traffic Management</strong> The integration of digital twins and artificial intelligence allows for real-time adjustments in traffic management to reduce congestion, thereby maximizing social benefits.</p></li><li><p><strong>China&#39;s Vehicle-Road-Cloud Integration + Real-time Digital Twin</strong> Essentially, this is a &quot;communication, sensing, and computation&quot; network. Its core goal is to digitally represent the real world in real time, seamlessly connecting all smart devices to provide real-time data support, helping traffic and other industries achieve more efficient collaboration, decision-making, and processing.</p><p>Specifically, by deploying AI-powered digital road base stations with &quot;communication, sensing, and computation&quot; capabilities at intersections, and combining them with AI roadside edge computing systems (AI-MRS), traffic managers can obtain dynamic information about all traffic participants within a 300-meter radius of the intersection. Based on this data, the system can instantly construct a digital twin model to provide precise, real-time services to all vehicles within the range, making traffic smarter and more efficient!</p><p><img src="/2024-China-Open-Source-Report/image/ossAI/B1fUbmzu1e.jpg" alt="中国智能化路车云"></p><blockquote><!----></blockquote></li></ul></li><li><p><strong>Medical Field</strong></p><ul><li><strong>Disease Diagnosis</strong> Spatial intelligence technology can perform 3D reconstruction and analysis of medical imaging data, helping doctors diagnose diseases more accurately. For example, by converting CT or MRI image data into three-dimensional models, it clearly shows the location, shape, and size of organs and lesions. This 3D information provides doctors with more intuitive and accurate diagnostic references, making complex cases easier to understand!</li><li><strong>Surgical Navigation and Decision Support</strong> Spatial intelligence technology also offers surgical navigation and decision support for doctors. Through 3D modeling and analysis of a patient&#39;s anatomical structure, doctors can gain a clear understanding of the surgical site&#39;s anatomy and vascular distribution. This intuitive three-dimensional view not only enhances the precision of surgical operations but also significantly improves surgical safety, leading to better treatment outcomes for patients!</li></ul></li></ul><h2 id="_6-embodied-intelligence" tabindex="-1">6. Embodied Intelligence <a class="header-anchor" href="#_6-embodied-intelligence" aria-label="Permalink to &quot;6. Embodied Intelligence&quot;">​</a></h2><p>There is growing interest and investment in combining AI with robotics, known as &quot;embodied intelligence.&quot; This term refers to giving robots both a &quot;body&quot; and &quot;intelligence.&quot; Currently, this technology has entered a new phase characterized by deep integration with AI technology, where embodied intelligent robots are considered the most important and promising application direction.</p><p>This chapter will explain embodied intelligence, focusing on main technologies and recent advancements in &quot;embodied intelligent robots.&quot; We will also delve into the industrial ecosystem behind it, its developmental trajectory, and the key players who are actively shaping this domain.</p><h3 id="_6-1-concepts-and-connotations" tabindex="-1">6.1 Concepts and connotations <a class="header-anchor" href="#_6-1-concepts-and-connotations" aria-label="Permalink to &quot;6.1 Concepts and connotations&quot;">​</a></h3><p>Embodied intelligence refers to the integration of artificial intelligence into various physical entities to form intelligent systems, enabling these entities to possess the capabilities of autonomous perception, learning, decision-making, and action within physical environments. This allows them to flexibly adapt to and accomplish tasks in such environments.</p><p>The essence of embodied intelligence is reflected in three key aspects: physical interaction, generalization and adaptation, and autonomous evolution. These aspects emphasize the dynamic and complex nature of designing and developing intelligent systems.</p><p><strong>Embodied Intelligence System Framework</strong><img src="/2024-China-Open-Source-Report/image/ossAI/en/ry5T7VQd1x.jpg" alt="embodied_intelligence_system_framework 1"></p><!----><h3 id="_6-2-key-technologies-and-progress" tabindex="-1">6.2 Key Technologies and Progress <a class="header-anchor" href="#_6-2-key-technologies-and-progress" aria-label="Permalink to &quot;6.2 Key Technologies and Progress&quot;">​</a></h3><p>The key to accelerating the development of embodied intelligent robots lies in technology, especially the rapid evolution of large AI models in recent times. Today&#39;s large models are not only rich in knowledge but also capable of understanding complex information, performing logical reasoning, and even &quot;self-learning and upgrading.&quot; These capabilities have provided a tremendous boost to the development of embodied intelligent robots.</p><p>In embodied intelligence, robots need to possess four core capabilities: &quot;perception, learning, decision-making, and action.&quot; In simple terms, they must be able to &quot;see, think, decide, and move.&quot; To achieve this efficient feedback loop, robots must seamlessly integrate various technologies, with different modules offering multiple technical paths. This technological synergy is like piecing together a puzzle to form a complete intelligent robot.</p><p><strong><!----></strong><img src="/2024-China-Open-Source-Report/image/ossAI/en/H1MqNNXdyg.png" alt="Key Technologies of Embodied Intelligent Robots"></p><!----><h3 id="_6-3-industry-ecology-and-composition" tabindex="-1">6.3 Industry Ecology and Composition <a class="header-anchor" href="#_6-3-industry-ecology-and-composition" aria-label="Permalink to &quot;6.3 Industry Ecology and Composition&quot;">​</a></h3><p>For embodied intelligent robots to adapt to a wide range of application scenarios, they require the support of various technologies and products, as well as the seamless integration of these complex systems. To ensure the healthy development of the entire industry ecosystem, it is essential for all market players to collaborate and work together, enabling the path forward to be both far-reaching and stable.</p><p><strong><!----></strong><img src="/2024-China-Open-Source-Report/image/ossAI/en/Syb-8NX_Je.png" alt="Embodied Intelligent Robot Industry Ecosystem"></p><!----><p>Embodied intelligent robots are like highly sophisticated machines, with hardware and software serving as their two main pillars. On the hardware side, this includes components, infrastructure, and the robot&#39;s physical body—these are its &quot;skeleton&quot; and &quot;muscles.&quot; On the software side, it represents its &quot;brain&quot; and &quot;nervous system,&quot; including the robot&#39;s operating system, simulation platforms, datasets, AI large models, visual perception algorithms, motion control algorithms, and more, all of which are key to bringing it &quot;to life.&quot;</p><p>Furthermore, the needs of different industries vary, and to allow robots to perform exceptionally in specific scenarios, application integration services are required. The establishment of standards and norms is akin to setting the &quot;rules of the game,&quot; ensuring that all parties can collaborate smoothly and collectively drive the development of embodied intelligent robots.</p><h3 id="_6-4-development-pathways-and-modalities" tabindex="-1">6.4 Development Pathways and Modalities <a class="header-anchor" href="#_6-4-development-pathways-and-modalities" aria-label="Permalink to &quot;6.4 Development Pathways and Modalities&quot;">​</a></h3><p>In the embodied intelligence industry chain, everyone is working together to drive its development. Currently, there are two main development directions for embodied intelligent robots:</p><ol><li><strong>Specialized Non-humanoid Robots</strong>: These robots, such as collaborative robots and commercial service robots, focus on specific tasks in professional scenarios. They are designed for particular purposes, like assistants on assembly lines or food delivery robots, dedicated to specialized tasks.</li><li><strong>Multifunctional Humanoid Robots</strong>: These robots are more like &quot;all-rounders.&quot; Using a humanoid form, their goal is to handle a wider range of general tasks and become versatile assistants that can be used both at home and in the workplace.</li></ol><p>In simple terms, one direction focuses on specialization, while the other pursues versatility.</p><p><strong><!----></strong><img src="/2024-China-Open-Source-Report/image/ossAI/en/ByKQd4mdyg.jpg" alt="Two Development Paths of Embodied Intelligent Robots-1"></p><!----><h3 id="_6-5-technology-provider-focus" tabindex="-1">6.5 Technology Provider Focus <a class="header-anchor" href="#_6-5-technology-provider-focus" aria-label="Permalink to &quot;6.5 Technology Provider Focus&quot;">​</a></h3><ul><li><p><strong>Focus on Well-Defined Pain Points</strong><br> Utilize industry experience to prioritize vertical application scenarios that have clear problems, urgent needs, and a clear commercialization prospect for in-depth exploration and technological optimization. In simple terms, start with the &quot;most pressing problems&quot; to achieve more with less effort.</p></li><li><p><strong>Leverage Data to Enhance R&amp;D</strong><br> Combine open-source data with the actual application data, user feedback, and industry insights accumulated in-house. These valuable &quot;first-hand materials&quot; can help you develop and train embodied intelligent robots more efficiently.</p></li><li><p><strong>Multiple Cooperation Models for Mutual Growth</strong><br> Explore different cooperation forms, such as technical collaboration, market expansion, or equity investment, to establish deep relationships with partners. This will help drive the efficient integration of hardware and software, such as collaboration between components and the robot&#39;s physical body, or AI models and the robot&#39;s body, thus accelerating real-world application.</p></li></ul><h3 id="_6-6-open-source-embodied-intelligent-robots" tabindex="-1">6.6 Open Source Embodied Intelligent Robots <a class="header-anchor" href="#_6-6-open-source-embodied-intelligent-robots" aria-label="Permalink to &quot;6.6 Open Source Embodied Intelligent Robots&quot;">​</a></h3><p>Currently, the field of embodied intelligent robots is rapidly advancing globally, particularly in China, where it has become a cutting-edge hotspot at the intersection of artificial intelligence and robotics. Embodied intelligent robots have not only achieved significant technological breakthroughs but are also beginning to make their mark in practical applications. Major tech companies and startups are entering the fray, launching innovative products tailored for industrial manufacturing, logistics, healthcare, education, home services, and more.</p><ul><li><strong>Development in China</strong> In China, the progress of embodied intelligent robots is particularly noteworthy. Leveraging strong manufacturing capabilities, diverse application scenarios, and an open technological ecosystem, Chinese companies are quickly rising to prominence in this field. Several Chinese enterprises are actively creating and participating in open-source projects, fostering technological sharing and innovation. Below are some open-source embodied intelligent robot manufacturers and their open-source status. Among them, <strong>Agibot</strong>, founded by Zhihui Jun (a former Huawei engineer), has emerged as a standout star in the industry.</li></ul><!----><br><ul><li><strong>Agibot&#39;s Open-Source Practices</strong> Agibot has significantly propelled the industry forward through its <strong>full-stack open-source approach</strong>. They have not only made their core algorithms and code accessible but also provided hardware design plans (Lingxi X1). Furthermore, they open-sourced AgiBot World, the world&#39;s first million-unit real-world dataset based on actual scenarios. This dataset surpasses existing similar datasets, such as Google&#39;s Open X-Embodiment, in both scale and quality, offering ten times the long-distance data volume and a hundred times the scene coverage. This initiative has created a comprehensive open-source ecosystem. Such openness allows more developers and companies to participate in the development of embodied intelligent robots at a lower cost, accelerating the dissemination and innovation of technology. <br></li></ul><p><img src="/2024-China-Open-Source-Report/image/ossAI/ryMdIB7_1e.png" alt="智元灵犀 X1 全栈开源机器人"></p><!----><br><ul><li><strong>Impact of Agibot&#39;s Full-Stack Open-Source Initiative</strong>: <ol><li><strong>Lowered Technical Barriers</strong>: By open-sourcing, more small and medium-sized enterprises as well as individual developers have the opportunity to enter the field of embodied intelligence, accelerating the diffusion of technology.</li><li><strong>Strengthened Community Ecosystem</strong>: Agibot has inspired a large number of developers to collaboratively optimize and refine related technologies, driving technological iteration across the industry.</li><li><strong>Innovation in Domestic Technology</strong>: The open-source model showcases the technical prowess of Chinese enterprises while enhancing China&#39;s influence in the global embodied intelligent robotics sector.</li><li><strong>Accelerated Diversified Applications</strong>: Through open-source initiatives, companies can quickly adapt technologies to meet the needs of various industries, facilitating the realization of product deployments.</li></ol></li></ul><p>The global embodied AI robotics industry is in its takeoff stage, and China is undoubtedly a key player in this transformation. The emergence of open-source pioneers like Zhiyuan Robotics has not only accelerated technological advancements but also set an example of open collaboration for the industry. In the future, as more enterprises and developers join the movement, embodied intelligent robots will demonstrate immense potential and value across a wide range of fields.</p><h3 id="_6-7-one-more-thing-open-source-generative-ai-enabling-embodied-intelligence-genesis-officially-released" tabindex="-1">6.7 One more thing! Open Source Generative AI Enabling Embodied Intelligence, Genesis Officially Released <a class="header-anchor" href="#_6-7-one-more-thing-open-source-generative-ai-enabling-embodied-intelligence-genesis-officially-released" aria-label="Permalink to &quot;6.7 One more thing! Open Source Generative AI Enabling Embodied Intelligence, Genesis Officially Released&quot;">​</a></h3><p><strong>Genesis</strong> is an embodied intelligence research platform that integrates generative model capabilities. This platform consists of a general-purpose physics engine, a robotics simulation platform, a photorealistic rendering system, and a data generation engine. The data generation engine leverages generative AI technology to convert natural language into training data for various modules.</p><p>The project was developed by a team led by Dr. Chuang Gan, Chief Scientist at the MIT-IBM Watson AI Lab. At the end of 2023, the team published a paper introducing RoboGen, a framework that uses generative AI to provide robots with unlimited learning data and fully automated training. This research gained global attention. After more than a year of development, the RoboGen framework was officially open-sourced as the Genesis embodied intelligence research platform, igniting widespread discussions across the internet.</p><p>According to <strong>OpenDigger</strong> data, since its release on December 19, 2024, the Genesis project attracted over 500 developers to join discussions within just 10 days. 21 contributors have already participated in the project, and it has received nearly 20,000 stars on GitHub. This is definitely a project worth keeping an eye on! <br></p><h2 id="_7-ai-agent" tabindex="-1">7. AI Agent <a class="header-anchor" href="#_7-ai-agent" aria-label="Permalink to &quot;7. AI Agent&quot;">​</a></h2><p>This chapter primarily references the <strong>&quot;Evolution and Impact of AI Agents White Paper&quot;</strong> released by the World Economic Forum in 2024, the <strong>&quot;China AI Agent Industry Research Report&quot;</strong> published by Jazz Year Think Tank, and the CSET workshop report. The goal is to provide readers with a glimpse into the rapidly evolving and continuously updated field of AI Agents.</p><h3 id="_7-1-course-of-evolution" tabindex="-1">7.1 Course of Evolution <a class="header-anchor" href="#_7-1-course-of-evolution" aria-label="Permalink to &quot;7.1 Course of Evolution&quot;">​</a></h3><p>Starting in the 1950s, the field has evolved from simple rule-based, deterministic systems to complex systems capable of handling uncertainty, learning, and adapting. This progress has been driven by advancements in computing power, the exponential growth of data, and breakthroughs in algorithms, including the development of large models and machine learning techniques.</p><p><img src="/2024-China-Open-Source-Report/image/ossAI/Bkti9PQ_kx.png" alt="image"></p><h3 id="_7-2-definition-and-core-components" tabindex="-1">7.2 Definition and Core Components <a class="header-anchor" href="#_7-2-definition-and-core-components" aria-label="Permalink to &quot;7.2 Definition and Core Components&quot;">​</a></h3><ul><li><p><strong>AI Agent</strong> is an autonomous system capable of perceiving its environment, making independent decisions, and taking actions to achieve specific goals. It consists of core components such as sensors, effectors, and a control center, enabling it to operate in both physical and digital environments. By receiving user input, perceiving the environment, planning decisions, and executing actions, it can effectively alter its surroundings.</p><p><img src="/2024-China-Open-Source-Report/image/ossAI/SkvhhPQ_Jx.png" alt="WEF-Core Components of an Agent"></p></li><li><p>2024 marks the breakout year for AI Agents, as the industry shifts from Copilot-driven models to deeper explorations of AI Agent capabilities. As a result, highly automated Copilot products are often generalized under the broader concept of AI Agents in the market. The key difference between Copilots and AI Agents lies in <strong>autonomous planning</strong>. Copilots require human direction, whereas AI Agents directly tackle target tasks with autonomous memory, reasoning, planning, and execution capabilities. In its ultimate form, an AI Agent would only need an initial user instruction and final feedback, operating independently without human intervention throughout the process.</p><p><img src="/2024-China-Open-Source-Report/image/ossAI/en/rJTMI_7OJl.png" alt="Chatbot-Copilot-Agent small"></p><blockquote><!----></blockquote></li></ul><br><ul><li>The main source of confusion between Copilot and AI Agent lies in the distinction between automation in the context of a workflow. From a results-oriented perspective, tasks can be infinitely broken down by humans, and some &quot;Copilot + automation&quot; solutions can fully automate individual work units. However, while these automated Copilot systems handle specific tasks independently, they still rely on human intervention to structure and direct workflows. In contrast, a true AI Agent autonomously orchestrates and executes an entire workflow without requiring manual task segmentation or oversight.</li></ul><table><thead><tr><th><strong>Name</strong></th><th><strong>Automation Implementation</strong></th><th><strong>Meaning</strong></th></tr></thead><tbody><tr><td><strong>Chatbot</strong></td><td>/</td><td>Humans perform the majority of tasks, similar to consulting AI for opinions or information. The AI provides information and suggestions but does not directly handle tasks.</td></tr><tr><td><strong>Copilot</strong></td><td>Achieves automation through complex prompts</td><td>Humans and AI collaborate, sharing the workload equally. The AI drafts work based on human prompts, while humans set goals, make adjustments, and finalize the output.</td></tr><tr><td><strong>Agent</strong></td><td>Achieves automation by setting goals</td><td>The AI handles the majority of tasks, with humans responsible for setting goals, providing resources, and overseeing results. The AI breaks down tasks, selects tools, manages progress, and autonomously concludes work upon achieving the goal.</td></tr><tr><td><br></td><td></td><td></td></tr></tbody></table><h3 id="_7-3-types-and-characteristics" tabindex="-1">7.3 Types and Characteristics <a class="header-anchor" href="#_7-3-types-and-characteristics" aria-label="Permalink to &quot;7.3 Types and Characteristics&quot;">​</a></h3><p>These include simple reflex agents, model-based reflex agents, goal-based agents, and utility-based agents, each with distinct decision-making methods and characteristics, ranging from straightforward rule mapping to complex multi-objective trade-off decisions. Currently, many AI Agent architectures are built on large language models and possess the capability for multi-component collaboration, such as AI Agents in automotive infotainment systems.</p><ol><li><p><strong>Advanced AI Agent</strong>: An AI entity with strong autonomy, built on large models and composed of multiple components, capable of handling complex tasks and continuously learning.</p><p><img src="/2024-China-Open-Source-Report/image/ossAI/SyFEsPQuyg.png" alt="image"></p></li><li><p><strong>AI Agent System</strong>: A system in which multiple AI agents with different functions work collaboratively to accomplish more complex tasks.</p></li><li><p><strong>Multi-Agent System (MAS)</strong>: A system that integrates multiple intelligent autonomous entities (including AI agents, AI agent systems, and human users) designed to interact and collaborate to achieve a common goal. Examples include autonomous driving systems. MAS consists of multiple independent agents or agent systems working together, with architectures that may involve networks and supervisory mechanisms. These systems enhance efficiency and capabilities but also face challenges in agent communication and interoperability, such as those encountered in traffic management applications.</p><p><img src="/2024-China-Open-Source-Report/image/ossAI/BkeDowm_yx.png" alt="image"></p></li><li><p><strong>The Structure and Relationship Between AI Agents, AI Agent Systems, and Multi-Agent Systems (MAS)</strong></p><p><img src="/2024-China-Open-Source-Report/image/ossAI/HJgpVRwQOyx.png" alt="WEF-Structure of a multi-Agent Systems"></p></li></ol><h3 id="_7-4-work-paradigms-are-being-upended" tabindex="-1">7.4 Work paradigms are being upended <a class="header-anchor" href="#_7-4-work-paradigms-are-being-upended" aria-label="Permalink to &quot;7.4 Work paradigms are being upended&quot;">​</a></h3><ul><li><p>The commercial value of AI Agents revolves around differences in work paradigms, and a shift in these paradigms signifies the beginning of a new era of intelligence.</p><p><strong><!----></strong><img src="/2024-China-Open-Source-Report/image/ossAI/en/SJera_Q_ke.png" alt="AI Agent Capability Requirements by Automation Level"></p><blockquote><p>Jazz Year Think Tank - China AI Agent Industry Research Report - AI Agent Automation Levels</p></blockquote></li><li><p><strong>Process-Oriented vs. Goal-Oriented</strong></p><p><img src="/2024-China-Open-Source-Report/image/ossAI/en/BJINR_QOkg.png" alt="The Enhancement of AI Agent Capabilities Drives the Shift in Work Paradigms"></p><blockquote><!----></blockquote></li></ul><br><h3 id="_7-5-areas-of-application" tabindex="-1">7.5 Areas of application <a class="header-anchor" href="#_7-5-areas-of-application" aria-label="Permalink to &quot;7.5 Areas of application&quot;">​</a></h3><p>AI Agents demonstrate broad application potential across multiple fields, including:</p><ul><li><strong>Workflow Automation</strong>: Enhancing enterprise efficiency.</li><li><strong>Personal Assistants</strong>: Providing personalized services.</li><li><strong>Healthcare</strong>: Improving patient experience.</li><li><strong>Education</strong>: Supporting personalized learning.</li><li><strong>...</strong></li></ul><p><img src="/2024-China-Open-Source-Report/image/ossAI/en/ryOnktX_1e.png" alt="Areas of application of AI Agent-small"></p><blockquote><!----></blockquote><h3 id="_7-6-ai-agent-ecosystem-mapping-in-china" tabindex="-1">7.6 AI Agent Ecosystem Mapping in China <a class="header-anchor" href="#_7-6-ai-agent-ecosystem-mapping-in-china" aria-label="Permalink to &quot;7.6 AI Agent Ecosystem Mapping in China&quot;">​</a></h3><ul><li>China&#39;s AI Agent market has already attracted a diverse range of participants, including <strong>internet giants, generative AI companies, enterprise SaaS providers, startups, and 3C (consumer electronics) companies</strong>. These companies are leveraging their <strong>technological expertise or industry knowledge</strong> to quickly enter the market, securing a strong position in the emerging ecosystem. Additionally, an increasing number of enterprises are actively refining their products and exploring various application scenarios.</li><li>As these pioneers gradually demonstrate their commercial value, the diversity of AI Agents in China will continue to expand, leading to an explosive growth in the number of enterprises investing in AI Agent technology.</li></ul><p><img src="/2024-China-Open-Source-Report/image/ossAI/en/r1juWY7OJg.png" alt="China AI Agent Ecosystem Map v1.0-small"></p><blockquote><!----></blockquote><ul><li>Some Noteworthy Internet Giants and Emerging Providers <ul><li>Baidu: Wenxin Agent Platform</li><li>ByteDance: Coze</li><li>Alibaba Cloud: DingTalk</li><li>Yonyou: Dayi</li><li>Huizhi Intelligent</li></ul></li><li>Some Noteworthy Overseas Companies <ul><li>OpenAI</li><li>IBM</li><li>AWS</li></ul></li></ul><h3 id="_7-7-impacts-and-response-measures" tabindex="-1">7.7 Impacts and Response Measures <a class="header-anchor" href="#_7-7-impacts-and-response-measures" aria-label="Permalink to &quot;7.7 Impacts and Response Measures&quot;">​</a></h3><p>The widespread adoption of AI Agents has the potential to enhance productivity and improve customer experiences across various fields, such as software development, healthcare, education, and finance. However, it also comes with technical, socioeconomic, and ethical risks, including system failures, excessive reliance on technology leading to shifts in employment structures, and ethical dilemmas. To ensure the safe, reliable, and beneficial development of AI Agents, it is crucial to implement mitigation strategies, such as technological improvements, public education, the establishment of ethical guidelines, and strengthened regulatory oversight.</p><h2 id="_8-the-compressed-21st-century" tabindex="-1">8. The Compressed 21st Century <a class="header-anchor" href="#_8-the-compressed-21st-century" aria-label="Permalink to &quot;8. The Compressed 21st Century&quot;">​</a></h2><p>At the end of this article, the editor would like to share a thought-provoking piece with you: <strong>&quot;<a href="https://darioamodei.com/machines-of-loving-grace" target="_blank" rel="noreferrer">Machines of Loving Grace</a>&quot;</strong>, a long-form essay published by Anthropic CEO Dario Amodei in October 2024. As we approach the 2025 Year of the Snake Spring Festival, let us reflect together on the potential prosperity and challenges that AI may bring to human society in the future.</p><p>In his article, <strong>Dario</strong> explores the potential impact of powerful artificial intelligence (AI) on the <strong>21st century</strong>, asserting that AI is poised to achieve significant breakthroughs across multiple domains.</p><p><strong>Biology &amp; Medicine</strong>: Dario predicts that AI will accelerate advancements in biology and medicine, achieving in 5 to 10 years what would have originally taken 50 to 100 years. He refers to this phenomenon as the <strong>&quot;Compressed 21st Century&quot;</strong>, meaning that AI will enable humanity to accomplish a century’s worth of biomedical progress in a remarkably short period.</p><p><strong>Neuroscience &amp; Mental Health</strong>: Advances in AI are expected to significantly improve the treatment of most mental illnesses, potentially even curing them. Additionally, AI could expand human cognitive and emotional capacities, enhancing what Dario describes as &quot;cognitive and psychological freedom.&quot;</p><p><strong>Economic Development &amp; Poverty Alleviation</strong>: Dario believes that AI can bring positive impacts to developing countries in areas such as healthcare, economy, and governance. However, he emphasizes that global collaboration is essential to ensure that these nations are not left behind.</p><p><strong>Peace &amp; Governance</strong>: Dario points out that if AI development follows the right trajectory, it could ultimately steer humanity toward rule of law, democracy, and enlightenment values. While this outcome is not guaranteed, statistical trends suggest that AI will accelerate progress in these directions, making the path clearer and the goals more defined.</p><p><strong>Work &amp; Life Meaning</strong>: The development of AI may bring challenges such as inequality, economic growth concerns, and ethical dilemmas, including the right to opt out of AI-driven systems. Nevertheless, Dario believes that AI has the potential to safeguard freedom, individual rights, and legal equality, ultimately contributing to the creation of a better world.</p><p>Finally, Dario emphasizes that while AI presents many challenges and risks, we should strive for positive outcomes that benefit everyone. He calls for unity and collective effort to navigate the challenges of the future together.</p></div></div></main><footer class="VPDocFooter" data-v-337c2e06 data-v-5f443411><!--[--><!--]--><div class="edit-info" data-v-5f443411><div class="edit-link" data-v-5f443411><a class="VPLink link vp-external-link-icon no-icon edit-link-button" href="https://github.com/kaiyuanshe/2024-China-Open-Source-Report/edit/main/en/ossAI.md" target="_blank" rel="noreferrer" data-v-5f443411><!--[--><svg xmlns="http://www.w3.org/2000/svg" viewbox="0 0 24 24" class="edit-link-icon" aria-label="edit icon" data-v-5f443411><path d="M18,23H4c-1.7,0-3-1.3-3-3V6c0-1.7,1.3-3,3-3h7c0.6,0,1,0.4,1,1s-0.4,1-1,1H4C3.4,5,3,5.4,3,6v14c0,0.6,0.4,1,1,1h14c0.6,0,1-0.4,1-1v-7c0-0.6,0.4-1,1-1s1,0.4,1,1v7C21,21.7,19.7,23,18,23z"></path><path d="M8,17c-0.3,0-0.5-0.1-0.7-0.3C7,16.5,6.9,16.1,7,15.8l1-4c0-0.2,0.1-0.3,0.3-0.5l9.5-9.5c1.2-1.2,3.2-1.2,4.4,0c1.2,1.2,1.2,3.2,0,4.4l-9.5,9.5c-0.1,0.1-0.3,0.2-0.5,0.3l-4,1C8.2,17,8.1,17,8,17zM9.9,12.5l-0.5,2.1l2.1-0.5l9.3-9.3c0.4-0.4,0.4-1.1,0-1.6c-0.4-0.4-1.2-0.4-1.6,0l0,0L9.9,12.5z M18.5,2.5L18.5,2.5L18.5,2.5z"></path></svg> Edit this page on GitHub<!--]--></a></div><div class="last-updated" data-v-5f443411><p class="VPLastUpdated" data-v-5f443411 data-v-238f58e4>Last updated: <time datetime="2025-08-30T09:01:32.000Z" data-v-238f58e4></time></p></div></div><nav class="prev-next" data-v-5f443411><div class="pager" data-v-5f443411><a class="VPLink link pager-link prev" href="/2024-China-Open-Source-Report/en/commercialization.html" data-v-5f443411><!--[--><span class="desc" data-v-5f443411>Previous page</span><span class="title" data-v-5f443411>OSS Commercialization</span><!--]--></a></div><div class="pager" data-v-5f443411><a class="VPLink link pager-link next" href="/2024-China-Open-Source-Report/en/open-source-milestones.html" data-v-5f443411><!--[--><span class="desc" data-v-5f443411>Next page</span><span class="title" data-v-5f443411>OSS Chronicle</span><!--]--></a></div></nav></footer><!--[--><!--]--></div></div></div><!--[--><!--]--></div></div><footer class="VPFooter has-sidebar" data-v-4709876f data-v-027126fb><div class="container" data-v-027126fb><p class="message" data-v-027126fb>Released under the CC BY-SA 4.0 License.</p><p class="copyright" data-v-027126fb>Copyright © 2014-present KAIYUANSHE</p></div></footer><!--[--><!--]--></div></div>
    <script>window.__VP_HASH_MAP__=JSON.parse("{\"readme.md\":\"BXx1ZJqG\",\"commercialization.md\":\"BJeobYan\",\"data.md\":\"DBiGo4lI\",\"en_commercialization.md\":\"DkxlW7_b\",\"en_data.md\":\"Bjzwpb5B\",\"en_index.md\":\"DzVkfeki\",\"en_open-source-milestones.md\":\"BQZY64-S\",\"en_ossai.md\":\"CnTjTpiv\",\"en_preface.md\":\"DkvIYRZW\",\"en_questionnaire.md\":\"DfP4ETZj\",\"en_readme.md\":\"D4piO13j\",\"index.md\":\"DSJHxmF_\",\"open-source-milestones.md\":\"C-RZyA5D\",\"ossai.md\":\"3O8caxSb\",\"preface.md\":\"BxgoykEa\",\"questionnaire.md\":\"BXmkmVxI\"}");window.__VP_SITE_DATA__=JSON.parse("{\"lang\":\"en-US\",\"dir\":\"ltr\",\"title\":\"VitePress\",\"description\":\"A VitePress site\",\"base\":\"/2024-China-Open-Source-Report/\",\"head\":[],\"router\":{\"prefetchLinks\":true},\"appearance\":true,\"themeConfig\":{\"logo\":\"/image/China-Open-Source-Report.png\",\"socialLinks\":[{\"icon\":\"github\",\"link\":\"https://github.com/kaiyuanshe/2024-China-Open-Source-Report\"},{\"icon\":{\"svg\":\"<?xml version=\\\"1.0\\\" encoding=\\\"UTF-8\\\"?><svg id=\\\"_图层_1\\\" xmlns=\\\"http://www.w3.org/2000/svg\\\" xmlns:xlink=\\\"http://www.w3.org/1999/xlink\\\" viewBox=\\\"0 0 248.7 143.1\\\"><defs><style>.cls-1{fill:url(#linear-gradient);}.cls-1,.cls-2{fill-rule:evenodd;}.cls-2{fill:#251714;}</style><linearGradient id=\\\"linear-gradient\\\" x1=\\\"135.9\\\" y1=\\\"69.1\\\" x2=\\\"112\\\" y2=\\\"14.8\\\" gradientUnits=\\\"userSpaceOnUse\\\"><stop offset=\\\"0\\\" stop-color=\\\"#0080ff\\\"/><stop offset=\\\"1\\\" stop-color=\\\"#00ebff\\\"/></linearGradient></defs><path class=\\\"cls-1\\\" d=\\\"m162.4,23.2c3,5.9,4.7,12.5,4.7,19.6s-.1,4-.4,6c-9.3-5.5-19.3-9-29.6-10.6l-6.2-14.9h31.5Zm-45.1,14.2c2.3-.2,4.7-.3,7-.3,2.4,0,4.7.1,7,.3l-3.9-12.3c-.5-1.4-1.8-2.1-3.1-2.1-1.3,0-2.6.7-3.1,2.1l-3.9,12.3h0Zm-31-14.2h31.5l-6.2,14.9c-10.3,1.6-20.3,5.2-29.6,10.6-.3-2-.4-4-.4-6,0-7,1.7-13.7,4.7-19.6h0Zm77.9,35.2c-7.5-7.9-16.2-13.2-25.3-16l12,28.9c1.1,2.6,2.5,2.9,4.2,1.2,3.9-4,7-8.8,9.1-14.1h0Zm-79.7,0c2.1,5.3,5.2,10.1,9.1,14.1,1.7,1.7,3.1,1.4,4.2-1.2l12-28.9c-9.1,2.8-17.8,8.1-25.3,16h0Zm60.6,21.8l-12.6-39.3c-2.7-.5-5.4-.7-8.2-.7-2.7,0-5.5.3-8.2.7l-12.6,39.3c6.2,3.4,13.2,5.4,20.8,5.4s14.6-1.9,20.8-5.4h0ZM124.3,0c13.7,0,25.8,6.4,33.7,16.4-10.9,2.6-22.2,3.9-33.7,3.9s-22.7-1.3-33.7-3.9C98.5,6.4,110.7,0,124.3,0Z\\\"/><path class=\\\"cls-2\\\" d=\\\"m27.6,102.9h5.2l-1.1-3.5h3.3l1.1,3.5h4.2v2.2h-7.2v2.7h6v14.3c0,.9-.7,1.6-1.6,1.6h-4.3v-2.2h2.6v-11.5h-2.8v1.5c0,4-.6,9.2-2.9,12.2h-3c2.2-4.1,2.6-8,2.6-11.9v-6.8h-2.2v-2.2h0Zm-26.7-3.1h22.8v2.2h-3.6v8.5h4.4v2.2h-4.4v11.1h-3.3v-11.1h-8.4c-.2,3.7-1.3,7.9-4.3,11.1H.4c3.1-3.2,4.5-7.2,4.7-11.1H0v-2.2h5.1v-8.5H.9v-2.2h0Zm16,2.2h-8.4v8.5h8.4v-8.5h0Zm213.8,0c-.8,2.8-2.1,5.2-3.8,7.2h-3.1c1.8-2.6,3.3-5.4,4.1-9.4h16.8c.8,4,2.3,6.7,4.1,9.4h-3.1c-1.8-2-3.1-4.4-3.8-7.2h-11.1Zm-.2,19.6h11.8l-1.6-4.3h3.4l2.4,6.5h-20.5l3.7-9.1h-3.9v-2.2h20.9v2.2h-13.4l-2.8,6.9h0Zm-1.5-14.6v2.2h14.5v-2.2h-14.5Zm-26.3-5c-.8,2.8-2.1,5.2-3.8,7.2h-3.1c1.8-2.6,3.3-5.4,4.1-9.4h16.8c.8,4,2.3,6.7,4.1,9.4h-3.1c-1.8-2-3.1-4.4-3.8-7.2h-11.1Zm14.9,14.6h-3.4l-1.7,5h-2.6v-6.9h8.8v-2.2h-8.8v-3.3h5.6v-2.2h-14.5v2.2h5.6v3.3h-8.8v2.2h8.8v6.9h-2.6l-1.7-5h-3.4l1.7,5h-4.6v2.2h24.5v-2.2h-4.6l1.7-5h0Zm-48.7-15.6h2.5v-1.6h3.3v1.6h11.2v-1.6h3.3v1.6h2.5v2.2h-2.5v9.3h3.4v2.2h-3.4c.6,1.8,1.4,3.4,2.8,5.3h-3.1c-1.4-1.4-2.4-3.4-3-5.3h-3.9v2h4v2.2h-4v2.6h10.6v2.2h-24.5v-2.2h10.6v-2.6h-4v-2.2h4v-2h-3.9c-.6,2-1.6,3.9-3,5.3h-3.1c1.4-1.9,2.3-3.5,2.8-5.3h-3.4v-2.2h3.4v-9.3h-2.5v-2.2h0Zm5.8,7.7h10.2v2.2h-10.2v1.7h11.2v-9.3h-11.2v1.7h10.2v2.2h-10.2v1.7h0Zm-28.1-8.8h18.6v2.2h-6.3l-.6,2.5h5.8v8.6c0,.9-.7,1.6-1.6,1.6h-3.2v7.4c0,.9-.7,1.6-1.6,1.6h-2.9v-2.2h1.3v-6.9h-4.8v-10.2h3.6l.6-2.5h-5.6v9.8c0,4-.5,8.9-2.3,12h-3c1.8-4,2-7.9,2-11.7v-12.3h0Zm-6.6,5.3l1.4,3.7h3.3l-1.4-3.7h-3.3Zm0-5.3l1.4,3.7h3.3l-1.4-3.7h-3.3Zm4.6,10.7h-3.3c0,5.4-.6,8.8-2.2,13.3h3.2c1.5-3.4,2.3-8,2.3-13.3h0Zm15.7,6c.1,2.6.7,5.4,2.1,7.3h3c-1.4-2.5-1.8-4.6-1.9-7.3h-3.2Zm-5.5,0h-3.2c-.1,2.7-.5,4.8-1.9,7.3h3c1.4-1.9,2-4.7,2.1-7.3h0Zm-.3-8h5.3v2.2h-5.3v1.8h6.3v-5.9h-6.3v1.8h0Zm-41.7-8.7h22.8v2.2h-3.6v8.5h4.4v2.2h-4.4v11.1h-3.3v-11.1h-8.4c-.2,3.7-1.3,7.9-4.3,11.1h-3.7c3.1-3.2,4.5-7.2,4.7-11.1h-5.1v-2.2h5.1v-8.5h-4.3v-2.2h0Zm16,2.2h-8.4v8.5h8.4v-8.5h0Zm-43.9-2.2h22.8v2.2l-8.8,6.8v1.7h9.6v2.2h-9.6v9.4c0,.9-.7,1.6-1.6,1.6h-8.1v-2.2h6.4v-8.9h-11.6v-2.2h11.6v-3.1l6.9-5.4h-17.7v-2.2h0Zm-27.2,0h23.3v2.2h-8.2l-.7,2.5h7.9v8.6c0,.9-.7,1.6-1.6,1.6h-5.3v7.4c0,.9-.7,1.6-1.6,1.6h-4v-2.2h2.3v-6.9h-6.9v-10.2h5.7l.7-2.5h-8.4v9.8c0,4-.5,8.9-2.3,12h-3c1.8-4,2-7.9,2-11.7v-12.3h0Zm17.8,16.6c.2,2.6.8,5.5,2.3,7.3h3.2c-1.5-2.5-2-4.6-2.2-7.3h-3.3Zm-8.5,0h-3.3c-.2,2.7-.7,4.8-2.2,7.3h3.2c1.5-1.9,2.1-4.7,2.3-7.3h0Zm-.8-8h9.2v2.2h-9.2v1.8h10.5v-5.9h-10.5v1.8h0Zm-17,15.3h4.3c-2.1-2-3.8-4-5-6,2.1-3.6,3.2-7.6,3.4-12.7h1.3v-2.2h-7.7c.2-1.2.3-2.4.3-3.5h-3.4c-.1,4-.6,6.8-2.3,10.7h2.1c.5,2.6,1.3,5.3,2.7,7.7-1.4,2.3-3.1,4.2-5,6h3.9c1.1-1,2-2.1,2.8-3.2.8,1.1,1.7,2.1,2.7,3.2h0Zm-4.6-17c.2-.5.3-1.1.4-1.6h3.5c-.3,3.5-.9,6.5-2,9.1-1-2.3-1.6-4.8-1.9-7.5h0ZM6,143.1c-1.2,0-2.2-.2-3.1-.7-.9-.5-1.6-1.1-2.1-2-.5-.9-.8-1.8-.8-2.9s.3-2.1.8-2.9c.5-.9,1.2-1.5,2.1-2,.9-.5,2-.7,3.1-.7s2.2.2,3.1.7,1.6,1.1,2.1,2c.5.9.8,1.8.8,2.9s-.3,2.1-.8,2.9c-.5.9-1.2,1.5-2.1,2-.9.5-2,.7-3.1.7h0Zm0-1.7c.7,0,1.3-.2,1.9-.5.5-.3,1-.8,1.3-1.4.3-.6.4-1.3.4-2.1s-.1-1.5-.4-2.1c-.3-.6-.7-1.1-1.3-1.4-.5-.3-1.2-.5-1.9-.5s-1.3.2-1.8.5c-.5.3-1,.8-1.3,1.4-.3.6-.4,1.3-.4,2.1s.1,1.5.4,2.1c.3.6.7,1.1,1.3,1.4.5.3,1.2.5,1.8.5h0Zm18.5-5.9c0,.7-.2,1.3-.5,1.9-.4.5-.9,1-1.5,1.3-.7.3-1.4.4-2.3.4h-2.3v3.8h-2.5v-10.9h4.8c.9,0,1.6.1,2.3.4.7.3,1.2.7,1.5,1.3.4.5.5,1.2.5,1.9h0Zm-2.5,0c0-.6-.2-1.1-.6-1.4-.4-.3-1-.5-1.8-.5h-1.8v3.9h1.8c.8,0,1.4-.2,1.8-.5.4-.3.6-.8.6-1.4h0Zm8.1-1.8v2.6h5.3v1.7h-5.3v3.1h5.6v1.7h-8.1v-10.9h8.1v1.7h-5.6Zm19.9-1.7v10.9h-2.3l-6.2-7.6v7.6h-2.3v-10.9h2.6l5.8,7.2v-7.2h2.2Zm11,8.1h-4.7l-1,2.8h-2.5l4.4-10.9h3.1l4.4,10.9h-2.7l-1-2.8h0Zm-.6-1.7l-1.7-4.7-1.7,4.7h3.4Zm15.4-4.7h-3.5v9.2h-2.5v-9.2h-3.5v-1.7h9.5v1.7h0Zm8.3,9.4c-1.2,0-2.2-.2-3.1-.7-.9-.5-1.6-1.1-2.1-2-.5-.9-.8-1.8-.8-2.9s.3-2.1.8-2.9c.5-.9,1.2-1.5,2.1-2,.9-.5,2-.7,3.1-.7s2.2.2,3.1.7c.9.5,1.6,1.1,2.1,2,.5.9.8,1.8.8,2.9s-.3,2.1-.8,2.9c-.5.9-1.2,1.5-2.1,2-.9.5-2,.7-3.1.7h0Zm0-1.7c.7,0,1.3-.2,1.8-.5.5-.3,1-.8,1.3-1.4.3-.6.4-1.3.4-2.1s-.1-1.5-.4-2.1c-.3-.6-.7-1.1-1.3-1.4-.5-.3-1.2-.5-1.8-.5s-1.3.2-1.8.5-1,.8-1.3,1.4c-.3.6-.4,1.3-.4,2.1s.1,1.5.4,2.1c.3.6.7,1.1,1.3,1.4.5.3,1.1.5,1.8.5h0Zm23.1,1.5h-2.5v-8.1l-3.2,8.1h-2.7l-3.2-8.1v8.1h-2.3v-10.9h3.5l3.3,8.6,3.3-8.6h3.6v10.9h0Zm15.6-9.2v2.7h5.1v1.7h-5.1v4.7h-2.5v-10.9h8v1.7h-5.5Zm14.4,9.4c-1.2,0-2.2-.2-3.1-.7-.9-.5-1.6-1.1-2.1-2-.5-.9-.8-1.8-.8-2.9s.3-2.1.8-2.9c.5-.9,1.2-1.5,2.1-2,.9-.5,2-.7,3.1-.7s2.2.2,3.1.7c.9.5,1.6,1.1,2.1,2,.5.9.8,1.8.8,2.9s-.3,2.1-.8,2.9c-.5.9-1.2,1.5-2.1,2-.9.5-2,.7-3.1.7h0Zm0-1.7c.7,0,1.3-.2,1.8-.5.5-.3,1-.8,1.3-1.4.3-.6.4-1.3.4-2.1s-.1-1.5-.4-2.1c-.3-.6-.7-1.1-1.3-1.4s-1.2-.5-1.8-.5-1.3.2-1.8.5c-.5.3-1,.8-1.3,1.4-.3.6-.4,1.3-.4,2.1s.1,1.5.4,2.1c.3.6.7,1.1,1.3,1.4.5.3,1.2.5,1.8.5h0Zm14.6,1.7c-1.1,0-2-.2-2.8-.5-.8-.3-1.4-.8-1.9-1.4-.4-.6-.7-1.4-.7-2.2v-7h2.3v6.8c0,.9.3,1.5.8,1.9.5.4,1.2.7,2.2.7s1.7-.2,2.2-.7c.5-.4.7-1.1.7-1.9v-6.8h2.5v7c0,.8-.2,1.6-.7,2.2-.4.6-1.1,1.1-1.9,1.4-.8.3-1.7.5-2.8.5h0Zm19.7-11.1v10.9h-2.3l-6.2-7.6v7.6h-2.3v-10.9h2.6l5.8,7.2v-7.2h2.2Zm14.6,5.5c0,1.1-.2,2.1-.7,2.9-.5.8-1.2,1.5-2,1.9-.9.4-1.9.7-3.1.7h-5.1v-10.9h5.1c1.2,0,2.2.2,3.1.7.9.5,1.6,1.1,2,1.9.5.8.7,1.8.7,2.9h0Zm-2.5,0c0-1.1-.3-2-1-2.7-.7-.7-1.6-1-2.8-1h-2.1v7.5h2.1c1.2,0,2.2-.3,2.8-1,.7-.7,1-1.6,1-2.7h0Zm12.8,2.7h-4.7l-1,2.8h-2.5l4.4-10.9h3.1l4.4,10.9h-2.7l-1-2.8h0Zm-.6-1.7l-1.7-4.7-1.7,4.7h3.4Zm15.4-4.7h-3.5v9.2h-2.5v-9.2h-3.5v-1.7h9.5v1.7h0Zm5.7-1.7v10.9h-2.5v-10.9h2.5Zm9.5,11.1c-1.2,0-2.2-.2-3.1-.7-.9-.5-1.6-1.1-2.1-2-.5-.9-.8-1.8-.8-2.9s.3-2.1.8-2.9c.5-.9,1.2-1.5,2.1-2,.9-.5,2-.7,3.1-.7s2.2.2,3.1.7c.9.5,1.6,1.1,2.1,2,.5.9.8,1.8.8,2.9s-.3,2.1-.8,2.9c-.5.9-1.2,1.5-2.1,2-.9.5-2,.7-3.1.7h0Zm0-1.7c.7,0,1.3-.2,1.8-.5.5-.3,1-.8,1.3-1.4.3-.6.4-1.3.4-2.1s-.1-1.5-.4-2.1c-.3-.6-.7-1.1-1.3-1.4-.5-.3-1.2-.5-1.8-.5s-1.3.2-1.8.5c-.5.3-1,.8-1.3,1.4-.3.6-.4,1.3-.4,2.1s.2,1.5.4,2.1c.3.6.7,1.1,1.3,1.4.5.3,1.2.5,1.8.5h0Zm20.1-9.4v10.9h-2.3l-6.2-7.6v7.6h-2.3v-10.9h2.6l5.8,7.2v-7.2h2.2Z\\\"/></svg>\"},\"link\":\"https://atomgit.com/kaiyuanshe/2024-China-Open-Source-Report\"}],\"search\":{\"provider\":\"local\"},\"externalLinkIcon\":true},\"locales\":{\"root\":{\"label\":\"简体中文\",\"lang\":\"zh-Hans\",\"title\":\"2024 中国开源年度报告\",\"description\":\"2024 中国开源年度报告\",\"themeConfig\":{\"nav\":[{\"text\":\"首页\",\"link\":\"/\"},{\"text\":\"往年年报\",\"link\":\"https://kaiyuanshe.feishu.cn/wiki/wikcnUDeVll6PNzw900yPV71Sxd\",\"target\":\"_blank\"}],\"sidebar\":[{\"items\":[{\"text\":\"卷首语\",\"link\":\"/preface\"},{\"text\":\"问卷篇\",\"link\":\"/questionnaire\"},{\"text\":\"数据篇\",\"link\":\"/data\"},{\"text\":\"商业化篇\",\"link\":\"/commercialization\"},{\"text\":\"开源人工智能篇\",\"link\":\"/ossAI\"},{\"text\":\"开源大事记\",\"link\":\"/open-source-milestones\"}]}],\"footer\":{\"message\":\"Released under the CC BY-SA 4.0 License.\",\"copyright\":\"Copyright © 2014-present KAIYUANSHE\"},\"editLink\":{\"pattern\":\"https://github.com/kaiyuanshe/2024-China-Open-Source-Report/edit/main/:path\",\"text\":\"在 GitHub 上编辑本页内容\"},\"lastUpdated\":{\"text\":\"更新于\"},\"docFooter\":{\"prev\":\"上一页\",\"next\":\"下一页\"}}},\"en\":{\"label\":\"English\",\"lang\":\"en-US\",\"title\":\"2024 COSR\",\"description\":\"2024 China Open Source Report\",\"themeConfig\":{\"nav\":[{\"text\":\"Home\",\"link\":\"/en\"},{\"text\":\"Annual report of previous years\",\"link\":\"https://kaiyuanshe.feishu.cn/wiki/wikcnUDeVll6PNzw900yPV71Sxd\",\"target\":\"_blank\"}],\"sidebar\":[{\"items\":[{\"text\":\"Preface\",\"link\":\"/en/preface\"},{\"text\":\"OSS Questionnaire\",\"link\":\"/en/questionnaire\"},{\"text\":\"OSS Data Analytics\",\"link\":\"/en/data\"},{\"text\":\"OSS Commercialization\",\"link\":\"/en/commercialization\"},{\"text\":\"OSS AI\",\"link\":\"/en/ossAI\"},{\"text\":\"OSS Chronicle\",\"link\":\"/en/open-source-milestones\"}]}],\"footer\":{\"message\":\"Released under the CC BY-SA 4.0 License.\",\"copyright\":\"Copyright © 2014-present KAIYUANSHE\"},\"editLink\":{\"pattern\":\"https://github.com/kaiyuanshe/2024-China-Open-Source-Report/edit/main/:path\",\"text\":\"Edit this page on GitHub\"}}}},\"scrollOffset\":134,\"cleanUrls\":false}");</script>
    
  </body>
</html>